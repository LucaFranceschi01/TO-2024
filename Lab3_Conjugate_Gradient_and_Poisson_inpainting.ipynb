{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RnhalJM-lVeX"
      },
      "source": [
        "# Group Members:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z4P5OKD6FsFK"
      },
      "source": [
        "<font color='blue'>**PUT NAMES OF YOUR TEAM MEMBERS HERE**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "*   Luca Franceschi - u199149\n",
        "*   Jan Corcho - u188244"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClpT8_NSjrH0"
      },
      "source": [
        "#Guide\n",
        "\n",
        "In this practice we will study the minimization of quadratic functions using two iterative methods: gradient descent and conjugate gradient method. We will start working first with problems in $\\mathbb R^2$, where we can compare visually the diffences between both algorithms.\n",
        "In the second part of the lab we will apply the same algorithms but this time to an image editing problem using a technique called *Poisson editing*.\n",
        "\n",
        "\\\\\n",
        "\n",
        "For any doubts before and after the practice, you can contact your teacher:\n",
        "\n",
        "Nneka Okolo - nnekamaureen.okolo@upf.edu\n",
        "\n",
        "Pablo Arias - pablo.arias@upf.edu\n",
        "\n",
        "Adriano Pastore - adriano.pastore@upf.edu\n",
        "\n",
        "\\\\\n",
        "\n",
        "**Deadlines**: See\n",
        "[P101](https://calendar.google.com/calendar/embed?src=c_b679939a9db8a1d8cd9f01f62d373d173f76794e4137c40e793a8d2cb11708f8%40group.calendar.google.com&ctz=Europe%2FMadrid/),\n",
        "[P102](https://calendar.google.com/calendar/embed?src=c_5a65338fe8c3ce7909e62bb6b572b1a61ff4ad3543b12f72468e1a16bca41bd0%40group.calendar.google.com&ctz=Europe%2FMadrid),\n",
        "[P201](https://calendar.google.com/calendar/embed?src=c_58aa336a0c5d0a38b13dd4a38071e7d8f9a18f4306ffeef2e48276087c339163%40group.calendar.google.com&ctz=Europe%2FMadrid),\n",
        "[P202](https://calendar.google.com/calendar/embed?src=c_dac1d492e1060f3cee35420a9c2ff0d345e89a002cc8c70fe74bf0b78bf99d37%40group.calendar.google.com&ctz=Europe%2FMadrid),\n",
        "\n",
        "\n",
        "\\\\\n",
        "\n",
        "**Submission instructions**\n",
        "\n",
        "Complete the code and answer the questions below.\n",
        "\n",
        "Export the notebook with the answers using the menu option File->Download .ipynb.\n",
        "\n",
        "Rename exported notebook with the format **lastnameUid.ipynb** where lastname is the first surname of **Member 1** in the form and Uid is their UPF ID.\n",
        "\n",
        "Submit your solution [here](https://forms.gle/AdYQwDEjAta1QaRY6) by the deadline. **Only one member needs to complete this step**.\n",
        "\n",
        "You will receive an acknowledgement of receipt.\n",
        "\n",
        "\\\\\n",
        "\n",
        "**Grading**:\n",
        "\n",
        "The evaluation is based on the report documenting your work (with figures), results, conclusions and the commented code together."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZZ5yRmAATA6"
      },
      "source": [
        "**Instructions for answering the questions.**\n",
        "\n",
        "Questions are indicated in blue. Some questions require answers in the form of text, some others require completing code. See the examples below. *Please do not modify the notebook outside of these cells.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZKCIcWMHBbqX"
      },
      "source": [
        "<font color='blue'>**(QUESTION 42)** Based on what you know at this moment, answer these questions:\n",
        "1. What are your favorite subjects?\n",
        "2. What are your favourite hobbies?\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TN82fq3jCP61"
      },
      "source": [
        "<font color='red'>**ANSWER**</font>\n",
        "\n",
        "1. I only like one subject: \"Optimization Techniques.\"\n",
        "1. I like writing equations $e^{i\\pi} + 1 = 0$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEZYuiviECbD"
      },
      "source": [
        "<font color='blue'>**(QUESTION 43)** This is a coding question. There is no <font color='red'>**ANSWER**</font> cell. Instead, you should complete the code cell following the question. Typically, you'll find TODOs in the code indicating the places that you are expected to complete.\n",
        "</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AbMOhxNZE7bC"
      },
      "outputs": [],
      "source": [
        "a = None     # TODO substitute the None by a nice number to print\n",
        "print(\"The number a is {}\".format(a))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZDTkeGKWlX6a"
      },
      "source": [
        "# Quadratic functions basics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1KgGyQOKv67h"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import cm\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from math import sqrt\n",
        "from IPython.display import clear_output, display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyKDnzgbv67m"
      },
      "source": [
        "<font color='blue'>**(QUESTION 1)** Which of the following matrices are positive, semipositive or negative definite?</font>\n",
        "\n",
        "$$\\textbf D_1 =\n",
        "\\left[\n",
        "\\begin{array}{c c}\n",
        "\t-2 & 0\\\\\n",
        "\t0 & 4\n",
        "\\end{array}\n",
        "\\right],\\quad \\textbf D_2 =\n",
        "\\left[\n",
        "\\begin{array}{c c}\n",
        "\t0 &0 \\\\ 0& 2\n",
        "\\end{array}\n",
        "\\right], \\quad \\textbf D_3 =\n",
        "\\left[\n",
        "\\begin{array}{c c}\n",
        "\t0.5 & 0 \\\\ 0 & 1.5\n",
        "\\end{array}\n",
        "\\right].\n",
        "$$\n",
        "\n",
        "$$\\textbf A_1 =\n",
        "\\left[\n",
        "\\begin{array}{c c}\n",
        "\t1 & -3 \\\\\n",
        "\t-3 & 1\n",
        "\\end{array}\n",
        "\\right],\\quad \\textbf A_2 =\n",
        "\\left[\n",
        "\\begin{array}{c c}\n",
        "\t1 &-1 \\\\ -1& 1\n",
        "\\end{array}\n",
        "\\right], \\quad \\textbf A_3 =\n",
        "\\left[\n",
        "\\begin{array}{c c}\n",
        "\t1 & -0.5 \\\\ -0.5 & 1\n",
        "\\end{array}\n",
        "\\right].\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmWmTuG6ogfh"
      },
      "source": [
        "<font color='red'> The ones that have only positive eigenvalues are positive definite, the ones that have only negative eigenvalues are negative definite and the ones that have both positive and zero eigenvalues are considered semipositive.</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGmEy7ACv67m"
      },
      "outputs": [],
      "source": [
        "# Define the matrices\n",
        "D1 = np.array([[-2, 0], [0, 4]])\n",
        "D2 = np.array([[0, 0], [0, 2]])\n",
        "D3 = np.array([[0.5, 0], [0, 1.5]])\n",
        "A1 = np.array([[1, -3], [-3, 1]])\n",
        "A2 = np.array([[1, -1], [-1, 1]])\n",
        "A3 = np.array([[1, -.5], [-.5, 1]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sDjpmW6Gv67n"
      },
      "outputs": [],
      "source": [
        "# TODO: Build up the code to demonstrate which matrices are\n",
        "#       positive definite and reasoning what you are implementing\n",
        "matrices = {\"D1\": D1, \"D2\": D2, \"D3\": D3, \"A1\": A1, \"A2\": A2, \"A3\": A3}\n",
        "for k, v in matrices.items():\n",
        "    eval, evec = np.linalg.eigh(v)\n",
        "    if np.all(eval > 0):\n",
        "        print(f'Matrix {k} is positive definite')\n",
        "    elif np.all(eval < 0):\n",
        "        print(f'Matrix {k} is negative definite')\n",
        "    elif np.all(eval >= 0):\n",
        "        print(f'Matrix {k} is positive semidefinite')\n",
        "    elif np.all(eval <= 0):\n",
        "        print(f'Matrix {k} is negative semidefinite')\n",
        "    else:\n",
        "        print(f'Matrix {k} is indefinite')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E9jNg5i1v67n"
      },
      "source": [
        "The Python script `quad_fun_main` plots contours of the quadratic functions $g_i(\\mathbf{x}) =\\langle \\mathbf{x}, \\mathbf{D}_i\\mathbf{x} \\rangle$ and $f_i(\\mathbf{x}) =\\langle \\mathbf{x}, \\mathbf{A}_i\\mathbf{x} \\rangle$ for  the  above  matrices.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HWS0GibAv67n"
      },
      "outputs": [],
      "source": [
        "def quad_fun_main(matrices: dict):\n",
        "\n",
        "    # Define the x1 and x2 axis and mesh to draw the level lines\n",
        "    x1 = np.arange(start = -10, stop = 10.1, step = 0.1)\n",
        "    x2 = np.arange(start = -10, stop = 10.1, step = 0.1)\n",
        "    X, Y = np.meshgrid(x1, x2)\n",
        "\n",
        "    # Calculate Z\n",
        "    x = np.array([X.T.flatten(), Y.T.flatten()])\n",
        "    results = {}\n",
        "    for key, matrix in matrices.items():\n",
        "        print(\"Procesing {}\".format(key))\n",
        "        aux = np.matmul(matrix, x)\n",
        "\n",
        "        result = np.zeros((x1.shape[0], x2.shape[0]))\n",
        "        for j in range(result.shape[0]):\n",
        "            for i in range(result.shape[1]):\n",
        "                result[i, j] = x[:, result.shape[0] * j + i].dot(aux[:, result.shape[0] * j + i])\n",
        "\n",
        "        if key.startswith(\"D\"):\n",
        "            matrix_name = \"G{}\".format(key[-1])\n",
        "        else:\n",
        "            matrix_name = \"F{}\".format(key[-1])\n",
        "        print(\"\\tSaving as {}\".format(matrix_name))\n",
        "        results[matrix_name] = result\n",
        "\n",
        "    # Plot the contours\n",
        "    fig, axs = plt.subplots(nrows = int(len(results) / 3),\n",
        "                        ncols = 3,\n",
        "                        figsize = (14,8))\n",
        "    for row in range(axs.shape[0]):\n",
        "        for column in range(axs.shape[1]):\n",
        "            matrix_name = list(results.keys())[axs.shape[0] * row + row + column]\n",
        "            Z = results.get(matrix_name)\n",
        "            axs[row, column].contour(X, Y, Z,\n",
        "                                     corner_mask = False, levels = 150,\n",
        "                                     linewidths=(1,), cmap = cm.coolwarm)\n",
        "            axs[row, column].set_title(matrix_name)\n",
        "\n",
        "    # Plot the surfaces\n",
        "    fig = plt.figure(figsize=(16, 10))\n",
        "    for row in range(axs.shape[0]):\n",
        "        for column in range(axs.shape[1]):\n",
        "            ax = fig.add_subplot(axs.shape[0], axs.shape[1], axs.shape[0] * row + row + column + 1, projection='3d')\n",
        "            matrix_name = list(results.keys())[axs.shape[0] * row + row + column]\n",
        "            Z = results.get(matrix_name)\n",
        "            ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,\n",
        "                               linewidth=0, antialiased=False)\n",
        "            ax.set_title(matrix_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XTfpGV1uv67o"
      },
      "outputs": [],
      "source": [
        "# Define the matrixes dictionary\n",
        "matrixes = {\"D1\": D1, \"D2\": D2, \"D3\": D3, \"A1\": A1, \"A2\": A2, \"A3\": A3}\n",
        "\n",
        "# Run the quad_fun_main\n",
        "quad_fun_main(matrixes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bfr6R93jxJOt"
      },
      "source": [
        "<font color='blue'>**(QUESTION\n",
        "2)** Based on the contours of each function answer the following questions.</font>\n",
        "\n",
        "<font color='blue'>- How  many  minima  does  each function have?    \n",
        "<font color='blue'>- Which is the relation between the $g_i$ and $f_i$?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b_CJN2YoA5SF"
      },
      "source": [
        "<font color='red'>**ANSWER**\n",
        "</font>\n",
        "\n",
        "Regarding the minima of the first 2 plots we can see that there are none, in line with the fact taht they are indefinite and semidefinite, while the last one is positive definite and has 1 minimum.\n",
        "Gi and Fi are representations of same matrices but they have different eigen vectors, wich roatate the functions and we end up looking at the a different prespective.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyRIfxklxzWP"
      },
      "source": [
        "# Minimization of quadratic functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XJe5Isqix5kW"
      },
      "source": [
        "\n",
        "Let us suppose that we have a quadratic function $f(\\textbf x) = \\frac12\\langle \\textbf x,\\textbf A \\textbf x\\rangle - \\langle \\textbf b,\\textbf x\\rangle$\n",
        "where $\\textbf x,\\textbf b\\in\\mathbb R^n$ and $\\textbf A$ is a $n\\times n$, symmetric, positive\n",
        "definite matrix. Let us compute its minimum as:\n",
        "\n",
        "$$\\nabla f(\\textbf x) = \\textbf A\\textbf x - \\textbf b = \\textbf r(\\textbf x),$$\n",
        "where we have defined $\\textbf r(\\textbf x)$ as the residual of $\\textbf x$.\n",
        "\n",
        "This implies that minimizing $f$ is equivalent to solving a linear equation.\n",
        "One could use direct methods such as Gaussian elimination or the pseudo-inverse\n",
        "to solve this equation (we have already seen an example in the\n",
        "regression problem of the\n",
        "Assignment 1).\n",
        "However, when the size of the problem grows, iterative methods are more\n",
        "efficient to find an (approximate) solution. In this Assignment we will see two iterative methods:\n",
        "the gradient descent method and the conjugate gradient."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NdzMYlB00e57"
      },
      "source": [
        "## Gradient descent method\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnrgAhDF0kMa"
      },
      "source": [
        "In the previous Assignment, we used the gradient descent method with a fixed step size.\n",
        "This time we will use an adaptive step size. The step size is chosen as follows. At\n",
        "iteration $k$ we have $\\textbf x_k$. The next iterate is defined as $\\textbf x_{k+1} =\n",
        "\\textbf x_k - \\alpha \\,\\textbf r_k$. To simplify notation we have defined $\\textbf r_k = \\textbf r(\\textbf x_k) = \\nabla f(\\textbf x_k)$. We will find the step\n",
        "size $\\alpha_k$ which minimizes the function $f$ restricted to the line $\\textbf x_k - \\alpha \\textbf r_k$:\n",
        "\n",
        "$$\\alpha_k = \\text{argmin}_\\alpha f(\\textbf x_k - \\alpha\\textbf r_k).$$\n",
        "\n",
        "This way of computing the step is called *exact* line search. In this context,\n",
        "$-\\textbf r_k$ is called the *search direction*.\n",
        "In the case of the gradient descent, the search direction is the negative gradient at point $\\textbf x_k$, but as we will see soon, there are\n",
        "better search directions. Before, let us see how we can compute the optimal step $\\alpha_k$ for a generic\n",
        "search direction $\\textbf p\\in\\mathbb R^n$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A11BH_Oj2rH0"
      },
      "source": [
        "As you say in theory lessons, the minimum of the function $h(\\alpha) = f(\\textbf x_k -\n",
        "\\alpha \\textbf p)$ is given by\n",
        "\n",
        "$$\\alpha_k = \\displaystyle \\frac{\\langle \\textbf p,(\\textbf A\\textbf x_k -\n",
        "\\textbf b)\\rangle}{\\langle \\textbf p,\\textbf A\\textbf p\\rangle} = \\displaystyle\\frac{\\langle \\textbf p,\\textbf r_k\\rangle}{\\langle \\textbf p,\\textbf A\\textbf p\\rangle}.$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NsUseODt37wt"
      },
      "source": [
        "<font color='blue'>**(QUESTION\n",
        "3)** Complete the code of the function `gradient_descent`. Follow the comments in the code.</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n-oZjp-AzvNN"
      },
      "outputs": [],
      "source": [
        "def gradient_descent(callback,\n",
        "                     b: np.matrix,\n",
        "                     callback_params: dict,\n",
        "                     x0: np.matrix,\n",
        "                     max_iters: int,\n",
        "                     tolerance: float,\n",
        "                     fig = None,\n",
        "                     ax = None):\n",
        "    \"\"\"\n",
        "    Implementation of the gradient descent algorithm (a\n",
        "    gradient descent scheme with optimal adaptive step size) for the minimization\n",
        "    of quadratic problems\n",
        "\n",
        "       f(x) = 1/2 x'Ax - bx.\n",
        "\n",
        "    It uses function handles. It requires a handle to a Python function that implements the product of matrix A with x.\n",
        "\n",
        "    :param callback: handle (pointer) to a Python function implementing the product with matrix A.\n",
        "    :param b: vector b, can be in matrix form (MxN)\n",
        "    :param callback_params: a dictionary with the callback function params\n",
        "    :param x0: initial condition, same dimensions as b (MxN)\n",
        "    :param max_iters: maximum number of iterations\n",
        "    :param tolerance: tolerance for the stopping condition (it stop when the norm of the gradient is below the tolerance)\n",
        "\n",
        "    :return x: value found (MxN)\n",
        "    :return fs: evolution of the target function (total_iters x 1 vector)\n",
        "    \"\"\"\n",
        "    x = x0\n",
        "    Ax = callback(x, **callback_params)\n",
        "    r = Ax - b\n",
        "    nr = np.linalg.norm(r)**2\n",
        "\n",
        "    # Note: since the variables can be stored as matrices (for example, x is an image)\n",
        "    #       we use np.multiply(x1, x2) to compute the dot products.\n",
        "\n",
        "    # Allocate memory for vector of energy values of the iterates\n",
        "    fs  = []\n",
        "\n",
        "\n",
        "    # Start loop\n",
        "    it = 0\n",
        "\n",
        "    while (sqrt(nr) > tolerance) and (it < max_iters):\n",
        "        # Increase iteration counter\n",
        "        it = it + 1\n",
        "        if it % 100 == 0:\n",
        "            print(\"[{} of {}]\\t-> |grad f(x)| = {}\".format(it, max_iters, np.sqrt(nr)))\n",
        "\n",
        "        # TODO: Compute quadratic energy f = .5 <Ax - b,x> - 0.5* <b,x>\n",
        "        value = 0.5 * np.multiply(r, x).sum() - 0.5 * np.multiply(b, x).sum()\n",
        "        fs.append(value)\n",
        "\n",
        "        # 1. line search in r - compute time step alpha\n",
        "        Ar = callback(r, **callback_params)\n",
        "        alpha = nr / (np.multiply(r, Ar).sum())\n",
        "\n",
        "        x_old = x # keep x_old - just for visualization\n",
        "\n",
        "        #2. TODO: update point x\n",
        "        x = x - alpha * r\n",
        "\n",
        "        # TODO: compute new residual r = Ax - b\n",
        "        Ax = callback(x, **callback_params)\n",
        "        r = Ax - b\n",
        "        nr = np.linalg.norm(r)**2\n",
        "\n",
        "        # ----- plot current position! Just for visualization purposes -----\n",
        "        if x.shape == (2, 1):\n",
        "            if not ax:\n",
        "                fig, ax = plt.subplots()\n",
        "            ax.plot(x[0, 0], x[1, 0], marker = 'o', color = \"k\")\n",
        "            ax.plot([x_old[0, 0], x[0, 0]], [x_old[1, 0], x[1, 0]], \"-k\")\n",
        "            ax.set_aspect('equal', 'box')\n",
        "            clear_output(wait=True)\n",
        "            display(fig)\n",
        "        elif x.shape[0] > 1 and x.shape[1] > 1:\n",
        "            # if x is a matrix (an image) (visualization of denoising)\n",
        "            if it % 10 == 0 or it == 1:\n",
        "                if not ax:\n",
        "                    fig, ax = plt.subplots(figsize = (12, 8))\n",
        "                ax.imshow(x, cmap = \"gray\")\n",
        "                clear_output(wait=True)\n",
        "                display(fig)\n",
        "\n",
        "    print(\"[{} of {}]\\t-> |grad f(x)| = {}\".format(it, max_iters, nr))\n",
        "    return x, np.matrix(fs).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gwcTQj44W5s"
      },
      "source": [
        "## Conjugate gradient method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QXgIv1k04ooh"
      },
      "source": [
        "The conjugate gradient method works by building a basis of $\\mathbb R^n$,\n",
        "$\\textbf d_1,\\textbf d_2,\\dots,\\textbf d_n$ of conjugate vectors with respect to $\\textbf A$. Two vectors\n",
        "$\\textbf v,\\textbf w\\in\\mathbb R^n$ are conjugate if $\\langle \\textbf v, \\textbf A \\textbf w\\rangle = 0$. It is a generalization of\n",
        "orthogonality (two vectors are orthogonal if they are conjugate with respect to\n",
        "the identity matrix!).\n",
        "\n",
        "Suppose that at iteration $k$ we have an iterate $\\textbf x_k$. The next iterate is defined as\n",
        "\n",
        "$$\\textbf x_{k+1} = \\textbf x_k - \\alpha_k \\textbf d_k.$$\n",
        "\n",
        "Here $-\\textbf d_k$ is a descent direction and $\\alpha_k$ is an adaptive time step,\n",
        "computed by optimal line search in the direction $-\\textbf d_k$\n",
        "\n",
        "$$\\alpha_k = \\frac{\\langle \\textbf d_k,\\textbf r_k\\rangle}{\\langle\\textbf d_k,\\textbf A\\textbf d_k\\rangle}.$$\n",
        "\n",
        "The descent direction is computed as a linear combination of the current\n",
        "gradient $\\nabla f(\\textbf x_k)  = \\textbf r_k$ and the previous descent direction\n",
        "$\\textbf d_{k-1}$,\n",
        "\n",
        "$$\\textbf d_k = -\\textbf r_k + \\beta_k \\textbf d_{k-1},$$\n",
        "\n",
        "where $\\beta_k$ is computed in such a way that the directions are a conjugate\n",
        "basis of $\\mathbb R^n$. It can be shown that $\\beta_k$ is given by\n",
        "\n",
        "$$\\beta_k = \\frac{\\| \\textbf r_k \\|^2}{\\|\\textbf r_{k-1}\\|^2}.$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvV6ZPLa6Xcr"
      },
      "source": [
        "<font color='blue'>**(QUESTION\n",
        "4)** Complete the code of the function `conjugate_gradient`. Follow the comments in the code.</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5NiiRH-t0KbY"
      },
      "outputs": [],
      "source": [
        "def conjugate_gradient(callback,\n",
        "                       b: np.matrix,\n",
        "                       callback_params: dict,\n",
        "                       x0: np.matrix,\n",
        "                       tolerance: float,\n",
        "                       max_iters: int,\n",
        "                       fig = None,\n",
        "                       ax = None):\n",
        "    \"\"\"\n",
        "    implementation of the conjugate gradient algorithm for the minimization of quadratic problems\n",
        "\n",
        "       f(x) = 1/2 x'Ax - bx.\n",
        "\n",
        "    It uses function handles. It requires a handle to a Python function that implements the product of matrix A with x.\n",
        "\n",
        "    :param callback: handle (pointer) to a Python function implementing the product with matrix A.\n",
        "    :param b: vector b, can be in matrix form (MxN)\n",
        "    :param callback_params: dictionary containing the params for the callback functions\n",
        "    :param x0: initial condition, same dimensions as b (MxN)\n",
        "    :param max_iters: maximum number of iterations\n",
        "    :param tolerance: tolerance for the stopping condition (it stop when the norm of the gradient is below the tolerance)\n",
        "\n",
        "    :return x: value found (MxN)\n",
        "    :return fs: evolution of the target function (total_iters x 1 vector)\n",
        "    \"\"\"\n",
        "\n",
        "    Ax = callback(x0, **callback_params)\n",
        "    r = Ax - b\n",
        "    nr = np.linalg.norm(r)**2\n",
        "    d = -r\n",
        "\n",
        "    # Note: since the variables can be stored as matrices (for example, x is an image)\n",
        "    #       we use np.multiply(x1, x2) to compute the dot products.\n",
        "\n",
        "    # Allocate memory for vector of energy values of the iterates\n",
        "    fs  = []\n",
        "    x = x0\n",
        "\n",
        "    # Start loop\n",
        "    it = 0\n",
        "\n",
        "    while (sqrt(nr) > tolerance) and (it < max_iters):\n",
        "        # Increase iteration counter\n",
        "        it = it + 1\n",
        "        if it % 10 == 0:\n",
        "            print(\"[{} of {}]\\t-> |grad f(x)| = {}\".format(it, max_iters, nr))\n",
        "\n",
        "        # Compute quadratic energy f = .5 <Ax - b,x> - 0.5* <b,x>\n",
        "        fs.append(0.5 * np.multiply(r, x).sum() - 0.5 * np.multiply(b, x).sum())\n",
        "\n",
        "        # TODO: line search in d - compute time step alpha\n",
        "        Ad = callback(d, **callback_params)\n",
        "        alpha = np.multiply(d, r).sum() / np.multiply(d, Ad).sum()\n",
        "\n",
        "        x_old = x # keep x_old - just for visualization\n",
        "\n",
        "        # TODO: update point x\n",
        "        x = x - alpha * d\n",
        "\n",
        "        # compute new residual r = Ax - b\n",
        "        Ax = callback(x, **callback_params)\n",
        "        r = Ax - b\n",
        "        nr_old = nr                       # inner product of the old residual\n",
        "        nr = np.linalg.norm(r)**2\n",
        "\n",
        "        # A-orthogonalization of r (Gram-Schmidt)\n",
        "        # new search direction d is a linear combination of r and previous d\n",
        "        # chosen so that it is A-orthogonal with the previous search directions\n",
        "        beta = nr / nr_old\n",
        "        d = -r + beta * d\n",
        "\n",
        "        # ----- plot current position! Just for visualization purposes -----\n",
        "        if x.shape == (2, 1):\n",
        "            if not ax:\n",
        "                fig, ax = plt.subplots()\n",
        "            ax.plot(x[0, 0], x[1, 0], marker = 'o', color = \"r\")\n",
        "            ax.plot([x_old[0, 0], x[0, 0]], [x_old[1, 0], x[1, 0]], \"-r\")\n",
        "            ax.set_aspect('equal', 'box')\n",
        "            clear_output(wait=True)\n",
        "            display(fig)\n",
        "        elif x.shape[0] > 1 and x.shape[1] > 1:\n",
        "            # if x is a matrix (an image) (visualization of denoising)\n",
        "            if it % 10 == 0:\n",
        "                if not ax:\n",
        "                    fig, ax = plt.subplots(figsize = (12, 8))\n",
        "                ax.imshow(x, cmap = \"gray\")\n",
        "                clear_output(wait=True)\n",
        "                display(fig)\n",
        "\n",
        "    print(\"[{} of {}]\\t-> |grad f(x)| = {}\".format(it, max_iters, nr))\n",
        "    return x, np.matrix(fs).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMUT2ByV68Ij"
      },
      "source": [
        "<font color='blue'>**(QUESTION\n",
        "5)** We start by visualizing the evolution of the gradient descent for a trivial problem in $\\mathbb R^2$.\n",
        "Run the following cell and answer these questions:</font>\n",
        "<font color='blue'>\n",
        "1. How many iterations does the gradient descent need to converge? Why?</font><br>\n",
        "<font color='blue'>\n",
        "1. How many iterations would the conjugate gradient need to converge? Why?</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_TAPpfvdBCes"
      },
      "source": [
        "<font color='red'>**ANSWER**\n",
        "In this case it takes 1 iteration. Since the \n",
        "</font>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxfKj-AGv67p"
      },
      "outputs": [],
      "source": [
        "# we define the callback function, simply implementing A*x\n",
        "def multiply_ax(x):\n",
        "    return A * x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6hTcAj_9v67t"
      },
      "outputs": [],
      "source": [
        "# --------------------------------------------------------------------------------------\n",
        "# create grid to plot contours of quadratic function -----------------------------------\n",
        "# --------------------------------------------------------------------------------------\n",
        "x1 = np.arange(start = -10, stop = 10.1, step = 0.1)\n",
        "x2 = np.arange(start = -10, stop = 10.1, step = 0.1)\n",
        "X, Y = np.meshgrid(x1, x2)\n",
        "\n",
        "# matrix containing all grid points as columns\n",
        "x = np.matrix([X.T.flatten(), Y.T.flatten()])\n",
        "\n",
        "# --------------------------------------------------------------------------------------\n",
        "# start with a trivial example: a quadratic function based on the identity matrix ------\n",
        "# --------------------------------------------------------------------------------------\n",
        "A = np.matrix([[1, 0], [0, 1]])\n",
        "b = np.matrix([[4], [3]])\n",
        "\n",
        "# evaluate function for all points in x\n",
        "Ax = A * x\n",
        "bx = b.T * x\n",
        "\n",
        "result = np.zeros((x1.shape[0], x2.shape[0]))\n",
        "for j in range(result.shape[0]):\n",
        "    for i in range(result.shape[1]):\n",
        "        result[i, j] = 0.5 * x[:, result.shape[0] * j + i].T * Ax[:, result.shape[0] * j + i] - bx[:, result.shape[0] * j + i]\n",
        "\n",
        "# Plot the contour of f\n",
        "fig = plt.figure()\n",
        "plt.contour(X, Y, result,\n",
        "            corner_mask = False, levels = 150,\n",
        "            linewidths=(1,), cmap = cm.coolwarm)\n",
        "plt.title('Level lines of f(x) = 1/2 x^T I x - bx')\n",
        "\n",
        "# run gradient descent\n",
        "tolerance = 1e-5\n",
        "max_iters = 1000\n",
        "x1 = gradient_descent(callback = multiply_ax,\n",
        "                      b = b,\n",
        "                      callback_params = {},\n",
        "                      x0 = np.matrix([[-7.5], [-7.5]]),\n",
        "                      tolerance = tolerance,\n",
        "                      max_iters = max_iters,\n",
        "                      fig = fig,\n",
        "                      ax = plt.gca())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4fwrlTOJv67u"
      },
      "source": [
        "<font color='blue'>**(QUESTION\n",
        "6)** Same as before, but for a more interesting quadratic function in $\\mathbb R^2$.\n",
        "Run the following cell and answer these questions:</font>\n",
        "<font color='blue'>\n",
        "1. How many iterations does the gradient descent need to converge?</font></br>\n",
        "<font color='blue'>\n",
        "1. Does the performance of the gradient descent depend on the position of the initial condition?</font></br>\n",
        "<font color='blue'>\n",
        "1. Are there any initial conditions for which the gradient descent converges in one iteration?</font></br>\n",
        "<font color='blue'>\n",
        "1. Which is the angle between consecutive descent directions for the gradient descent? Why?</font></br>\n",
        "<font color='blue'>\n",
        "1. How many iterations would the conjugate gradient need to converge? Why?</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcjTFSqlBDJL"
      },
      "source": [
        "<font color='red'>**ANSWER**\n",
        "</font>\n",
        "1. One\n",
        "2. Yes it depends\n",
        "3. \n",
        "4.\n",
        "5."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JlRjoZLAv67u"
      },
      "outputs": [],
      "source": [
        "# --------------------------------------------------------------------------------------\n",
        "# create grid to plot contours of quadratic function -----------------------------------\n",
        "# --------------------------------------------------------------------------------------\n",
        "x1 = np.arange(start = -10, stop = 10.1, step = 0.1)\n",
        "x2 = np.arange(start = -10, stop = 10.1, step = 0.1)\n",
        "X, Y = np.meshgrid(x1, x2)\n",
        "\n",
        "# matrix containing all grid points as columns\n",
        "x = np.matrix([X.T.flatten(), Y.T.flatten()])\n",
        "\n",
        "# --------------------------------------------------------------------------------------\n",
        "# a more general quadratic function ----------------------------------------------------\n",
        "# --------------------------------------------------------------------------------------\n",
        "A = np.matrix([[1, .9], [.9, 1]])\n",
        "b = np.matrix([[1], [2]])\n",
        "\n",
        "# evaluate function for all points in x\n",
        "Ax = A * x\n",
        "bx = b.T * x\n",
        "\n",
        "result = np.zeros((x1.shape[0], x2.shape[0]))\n",
        "for j in range(result.shape[0]):\n",
        "    for i in range(result.shape[1]):\n",
        "        result[i, j] = 0.5 * x[:, result.shape[0] * j + i].T * Ax[:, result.shape[0] * j + i] - bx[:, result.shape[0] * j + i]\n",
        "\n",
        "# Plot the contour of f\n",
        "fig = plt.figure()\n",
        "ax = plt.gca()\n",
        "plt.contour(X, Y, result,\n",
        "            corner_mask = False, levels = 150,\n",
        "            linewidths=(1,), cmap = cm.coolwarm)\n",
        "plt.title('Level lines of f(x) = 1/2 x^T I x - bx')\n",
        "\n",
        "# run gradient descent\n",
        "tolerance = 0.00001\n",
        "max_iters = 1000\n",
        "x1 = gradient_descent(callback = multiply_ax,\n",
        "                      b = b,\n",
        "                      callback_params = {},\n",
        "                      x0 = np.matrix([[5], [-5]]),  # TRY several starting points\n",
        "                      tolerance = tolerance,\n",
        "                      max_iters = max_iters,\n",
        "                      fig = fig,\n",
        "                      ax = ax)\n",
        "\n",
        "# run gradient descent\n",
        "tolerance = 0.00001\n",
        "max_iters = 100\n",
        "x1 = conjugate_gradient(callback = multiply_ax,\n",
        "                        b = b,\n",
        "                        callback_params = {},\n",
        "                        x0 = np.matrix([[5], [-5]]),  # TRY several starting points\n",
        "                        tolerance = tolerance,\n",
        "                        max_iters = max_iters,\n",
        "                        fig = fig,\n",
        "                        ax = ax)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTNvGK1i_xi4"
      },
      "source": [
        "<font color='blue'>**(QUESTION\n",
        "7)** This time we minimize a quadratic function in $\\mathbb R^{100}$.\n",
        "Run the following cell and answer these questions:</font>\n",
        "<font color='blue'>\n",
        "1. Which iterative method converges faster?\n",
        "</font></br>\n",
        "<font color='blue'>\n",
        "1. Which is the order of convergence of the gradient descent?\n",
        "</font></br>\n",
        "<font color='blue'>\n",
        "1. Why is the logarithmic plot of the error for the gradient descent linear?\n",
        "</font></br>\n",
        "<font color='blue'>\n",
        "1. Which is the maximum number of iterations needed for the conjugate gradient?</font></br>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I_W2oiLcBD8I"
      },
      "source": [
        "<font color='red'>**ANSWER**\n",
        "</font>\n",
        "\n",
        "1. Conjugate gradient tipycally converges at most in n iterations, in this case 2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6O5kSC6v67u"
      },
      "outputs": [],
      "source": [
        "# Define the problem matrix. Is it symmetric? Is it positive definite?\n",
        "A = np.matrix(np.ones(shape = (100, 100)) + np.diag(np.arange(0, 100)))\n",
        "b = np.matrix(np.ones(shape = (100, 1)))\n",
        "\n",
        "# run gradient descent\n",
        "tolerance = 10 ** -5\n",
        "max_iters = 10 ** 6\n",
        "x1, fs_1 = gradient_descent(callback = multiply_ax,\n",
        "                      b = b,\n",
        "                      callback_params = {},\n",
        "                      x0 = np.zeros_like(b),  # TRY several starting points\n",
        "                      tolerance = tolerance,\n",
        "                      max_iters = max_iters)\n",
        "\n",
        "# run gradient descent\n",
        "tolerance = 10 ** -5\n",
        "max_iters = 100\n",
        "x2, fs_2 = conjugate_gradient(callback = multiply_ax,\n",
        "                        b = b,\n",
        "                        callback_params = {},\n",
        "                        x0 = np.zeros_like(b),  # TRY several starting points\n",
        "                        tolerance = tolerance,\n",
        "                        max_iters = max_iters)\n",
        "\n",
        "m = -0.5 * b.T * np.linalg.lstsq(A, b)[0]\n",
        "m = m[0,0]\n",
        "\n",
        "fig, ax = plt.subplots(figsize = (14, 8))\n",
        "ax.semilogy(fs_1 - m)\n",
        "ax.semilogy(fs_2 - m, 'r')\n",
        "ax.legend(['gradient descent', 'conjugate gradient'])\n",
        "ax.set_title('logarithmic plot of f(x_i) - f^*')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kk7ggRivlmBU"
      },
      "source": [
        "# Poisson image editing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Py45wdyEFQxt"
      },
      "source": [
        "<img src=\"https://github.com/Muchay/OptTechCourse_Aux/blob/main/Lab3/images/lisa-ginevra-compositing-gray-levels.png?raw=true\"/>\n",
        "\n",
        "An image editing problem. We would like to swap the faces of Ginevra de' Benci (left) and Lisa Gherardini. The image on the right shows the result of composing images by editing the gray levels using the binary mask $m$ (3rd image from the left). The result shows a discontinuity of the gray levels along the boundary of the mask. In the next cells we're going to solve this problem using a technique called *Poisson editing*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pET6fUVN5PPG"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "%cd \"/content/drive/MyDrive/upf-optimization-techniques/Labs_2024/Lab3/jupyter\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ntdJCPznGn0B"
      },
      "outputs": [],
      "source": [
        "# By now, you should have the folder \"OptTechCourse_Aux\" in your drive\n",
        "# Create source path\n",
        "source = \"/content/drive/MyDrive/OptTechCourse_Aux/Lab3/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B0HxLuEV3kYb"
      },
      "outputs": [],
      "source": [
        "# Install if required\n",
        "# !python -m pip install opencv-python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woz0wXcw3kYh"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from PIL import Image\n",
        "import os\n",
        "import cv2 as cv\n",
        "import matplotlib.pyplot as plt\n",
        "from math import sqrt\n",
        "from IPython.display import clear_output, display"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KN2I7tP_3kYm"
      },
      "source": [
        "First we load the images. For better looking results, we will also blur them a bit with a Gaussian filter. Learn more about Gaussian blurring [here](https://docs.opencv.org/master/d4/d13/tutorial_py_filtering.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FoDtdzek3kYn"
      },
      "outputs": [],
      "source": [
        "# Define the images directory\n",
        "images_dir = os.path.abspath(source + \"images\")\n",
        "\n",
        "# Read all the required images\n",
        "ginevra = Image.open(os.path.join(images_dir, \"ginevra.png\"))\n",
        "lisa = Image.open(os.path.join(images_dir, \"lisa.png\"))\n",
        "mask = Image.open(os.path.join(images_dir, \"mask.png\"))\n",
        "\n",
        "# Convert all the images to 2D\n",
        "u1 = np.array(ginevra, dtype = float)[:, :, 0]\n",
        "u2 = np.array(lisa, dtype = float)[:, :, 0]\n",
        "mask = np.array(mask, dtype = float) / 255\n",
        "\n",
        "# Apply Gaussian blurring\n",
        "u1 = cv.GaussianBlur(src = u1, ksize = (9, 9), sigmaX = 2, borderType = cv.BORDER_REFLECT)\n",
        "u2 = cv.GaussianBlur(src = u2, ksize = (11, 11), sigmaX = 4, borderType = cv.BORDER_REFLECT)\n",
        "\n",
        "# Show all the images\n",
        "fig, (ax1, ax2, ax3) = plt.subplots(nrows = 1, ncols = 3, figsize = (12, 8))\n",
        "ax1.imshow(u1, cmap = \"gray\")\n",
        "ax1.set_title(\"Ginevra\")\n",
        "ax2.imshow(u2, cmap = \"gray\")\n",
        "ax2.set_title(\"Lisa\")\n",
        "ax3.imshow(mask, cmap = \"gray\")\n",
        "ax3.set_title(\"Mask\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lPkklc0kJTrW"
      },
      "source": [
        "## A quadratic energy for image ediding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J85QReUvJcq5"
      },
      "source": [
        "As an application of the conjugate gradient method for minimizing quadratic\n",
        "energies, we will present a method for image edition called Poisson editing.\n",
        "Poisson editing is based on editing the gradient of an image instead of working\n",
        "directly with the gray levels (or with the colors). As we will see, this\n",
        "simplifies many things for certain editing tasks.\n",
        "\n",
        "We will focus on the following problem: we have two famous portraits by\n",
        "Leonardo Da Vinci, and we would like to interchange the faces of Ginevra de'\n",
        "Benci and Lisa Gherardini. Both portraits have been aligned and scaled, so that\n",
        "the faces are the same size and are in the same position of the image. There is\n",
        "also a mask, which selects the part of the image we want to interchange.\n",
        "\n",
        "Let $u_1,u_2:\\Omega\\rightarrow \\mathbb R$ and $m:\\Omega\\rightarrow \\{0,1\\}$ be both\n",
        "images, and the binary mask. $\\Omega$ is a discrete rectangular 2D grid, the\n",
        "positions of the image pixels: $\\Omega = \\{1,\\dots,M\\}\\times\\{1,\\dots,N\\}$ ($M$\n",
        "columns and $N$ rows). Say we want the face of $u_1$ in $u_2.$ Let us call $u$\n",
        "the resulting image.\n",
        "\n",
        "\n",
        "**Editing directly the gray levels**\n",
        "\n",
        "The simplest thing to try is to define $u$ as follows:\n",
        "\n",
        "$$u_{ij} = m_{ij}u_{1,ij} + (1 - m_{ij})u_{2,ij}, \\quad \\text{ for\n",
        "}\\quad(i,j)\\in\\Omega.$$\n",
        "\n",
        "This gives the result shown in the figure above. Notice that the editing\n",
        "region is clearly visible! The reason is that one of the images is darker than\n",
        "the other, and this creates a discontinuity with the shape of the mask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmwGFepd3kYq"
      },
      "source": [
        "<font color='blue'>**(QUESTION\n",
        "8)** Complete the following code cell\n",
        "to implement the crude composition of gray levels.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xxueyDxE3kYs"
      },
      "outputs": [],
      "source": [
        "def composite_gray_levels(u1: np.array,\n",
        "                          u2: np.array,\n",
        "                          mask: np.array):\n",
        "    \"\"\"\n",
        "    Creates a composite image by copying a region of image 1 into image 2.\n",
        "    The region is determined as the pixels where the binary mask is 1.\n",
        "\n",
        "    :param u1: grayscale image 1 (MxN)\n",
        "    :param u2: grayscale image 2 (MxN)\n",
        "    :param mask: binary image 2 (MxN)\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO compute composite image\n",
        "    ucomp = None\n",
        "\n",
        "    return ucomp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g5Jq0eUwm1Ev"
      },
      "outputs": [],
      "source": [
        "# Show the composed image\n",
        "ucomp = composite_gray_levels(u1,u2,mask)\n",
        "plt.figure(figsize = (12, 8))\n",
        "plt.imshow(ucomp, cmap = \"gray\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHY8U5ui6U8n"
      },
      "source": [
        "**Editing the gradients**\n",
        "\n",
        "A better alternative is to perform the editing by copying *diferences of gray levels* instead of gray levels. Thus, first we will compute the *finite diffence gradient* of both\n",
        "images,\n",
        "\n",
        "$$\\nabla^+ u_{1,ij} = \\left[\n",
        "\\begin{array}{c}\n",
        "\t\\nabla^+_i u_{1,ij}\\\\\n",
        "\t\\nabla^+_j u_{1,ij}\\\\\n",
        "\\end{array}\n",
        "\\right]\\quad\\quad\\text{ and }\\quad\\quad\n",
        "\\nabla^+ u_{2,ij} = \\left[\n",
        "\\begin{array}{c}\n",
        "\t\\nabla^+_i u_{2,ij}\\\\\n",
        "\t\\nabla^+_j u_{2,ij}\\\\\n",
        "\\end{array}\n",
        "\\right]\n",
        "$$\n",
        "for $i=1,2,\\dots M,j=1,2,\\dots N.$\n",
        "Here $\\nabla^+_i$ and $\\nabla^+_j$ refer to the forward differences partial\n",
        "derivatives in the direction of $i$ (rows) and $j$ (columns), as we defined them\n",
        "in the previous assignment.\n",
        "\n",
        "We define a new vector-valued image by composing these gradients as follows:\n",
        "when $m_{ij} = 0$ (non-face pixel) we use the gradient of $u_2$, and when\n",
        "$m_{ij} = 1$ (face pixel) we use the gradient of $u_1$. We store this in a new\n",
        "vector valued image $v:\\Omega\\rightarrow \\mathbb R^2$:\n",
        "\\begin{align*}\n",
        "v_{ij} &= m_{ij}\\nabla^+ u_{1,ij} + (1-m_{ij})\\nabla^+ u_{2,ij}\\\\\n",
        "&=\\left[\\begin{array}{c}\n",
        "\tm_{ij}\\nabla^+_i u_{1,ij} + (1-m_{ij})\\nabla^+_i u_{2,ij}\\\\\n",
        "\tm_{ij}\\nabla^+_j u_{1,ij} + (1-m_{ij})\\nabla^+_j u_{2,ij}\\\\\n",
        "\\end{array}\\right]\\\\\n",
        "&=\\left[\\begin{array}{c}\n",
        "\tv_{1,ij}\\\\\n",
        "\tv_{2,ij}\\\\\n",
        "\\end{array}\\right]\n",
        "\\end{align*}\n",
        "for $(i,j)\\in\\Omega$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sdrHkAYM9v_-"
      },
      "source": [
        "For computing the forward difference gradient we will use `im_fwd_gradient` from previous assignments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Art8dnh83kYv"
      },
      "outputs": [],
      "source": [
        "def im_fwd_gradient(image: np.ndarray):\n",
        "    \"\"\"\n",
        "    Discrete gradient of an image using forward differences, with\n",
        "    homogeneous Neuman boundary conditions.\n",
        "\n",
        "    :param image: an MxN image\n",
        "\n",
        "    :return grad_i: partial derivative in the i direction (vertical)\n",
        "    :return grad_j: partial derivative in the j direction (horizontal)\n",
        "    \"\"\"\n",
        "    # TODO: Get the size of the image\n",
        "    image_shape = None ### WRITE YOUR SOLUTION\n",
        "\n",
        "    # TODO: Calculate both gradients\n",
        "    #       Check the Neuman boundary conditions\n",
        "    grad_i = None ### WRITE YOUR SOLUTION\n",
        "    grad_j = None ### WRITE YOUR SOLUTION\n",
        "    return grad_i, grad_j"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RrBav4Zq-HML"
      },
      "source": [
        "<font color='blue'>**(QUESTION\n",
        "9)** Complete the following code cell\n",
        "to compute the gradient composition $v$.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MmM1vFwJ3kYw"
      },
      "outputs": [],
      "source": [
        "def composite_gradients(u1: np.array,\n",
        "                        u2: np.array,\n",
        "                        mask: np.array):\n",
        "    \"\"\"\n",
        "    Creates a vector field v by combining the forward gradient of u1 and u2.\n",
        "    For pixels where the mask is 1 v coincides with the gradient of u1, and when\n",
        "    mask is 0 v coincidies with the gradient of u2.\n",
        "\n",
        "    :param u1: grayscale image 1 (MxN)\n",
        "    :param u2: grayscale image 2 (MxN)\n",
        "    :param mask: binary image 2 (MxN)\n",
        "\n",
        "    :return vi: composition of i components of gradients (vertical component)\n",
        "    :return vj: composition of j components of gradients (horizontal component)\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: Compute the gradients of u1 and u2, and define a new gradient [vi,vj]\n",
        "    # which corresponds to the gradient of u1 when mask = 1, and the gradient\n",
        "    # of u2 when mask = 0\n",
        "\n",
        "    vi = None\n",
        "    vj = None\n",
        "\n",
        "    return vi, vj"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJbV4fJDAb-V"
      },
      "outputs": [],
      "source": [
        "vi, vj = composite_gradients(u1, u2, mask)\n",
        "\n",
        "plt.figure(figsize = (24, 8))\n",
        "plt.subplot(1,2,1)\n",
        "plt.imshow(vi, cmap = \"gray\")\n",
        "plt.subplot(1,2,2)\n",
        "plt.imshow(vj, cmap = \"gray\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpLvu6AH6V31"
      },
      "source": [
        "Now we have to find an image $u$ whose gradient coincides (or is similar) with $v$:\n",
        "$\\nabla^+ u \\approx v$. In addition, the image should be $u_2$ outside the face (when\n",
        "$m_{ij} = 0$). We will find $u$ by solving a quadratic energy which imposes these too conditions as follows:\n",
        "\n",
        "$$E(u) = \\overbrace{\\dfrac{1}{2}\\sum_{i = 1}^N\\sum_{j = 1}^M |\\nabla^+u_{ij} - v_{ij}|_{\\mathbb R^2}^2}^{\\text{gradients similar to $v$}} +\n",
        "\\overbrace{\\dfrac{1}{2}\\sum_{i = 1}^N\\sum_{j = 1}^M \\beta_{ij}(u_{ij} - u_{2,ij})^2}^{\\text{$u$ is $u_{2}$ outside the mask}}$$\n",
        "\n",
        "where $\\beta:\\Omega\\rightarrow \\mathbb R$ is a coefficients image which controls the\n",
        "attachment to $u_{2,ij}$. We will use $\\beta_{ij} = \\beta_0 (1- m_{ij})$, where\n",
        "$\\beta_0 \\in \\mathbb R$ is a constant. This means that $\\beta_{ij}$ is zero on\n",
        "the face region, only acting outside the face. Recall that $|\\cdot|$ denotes\n",
        "the Euclidean norm in $\\mathbb R^2$.\n",
        "\n",
        "Similar to what we did for denoising, we can express the energy in matrix\n",
        "notation, using our vector representation of images and discrete gradients:\n",
        "\n",
        "$$\n",
        "\tE(u) = \\frac12\\langle \\nabla^+ u - v, \\nabla^+ u - v\\rangle_{\\mathcal Y} + \\frac12\\langle B (u\n",
        "\t- u_2), u - u_2\\rangle_{\\mathcal X}.\n",
        "$$\n",
        "\n",
        "where $B$ is a $MN\\times MN$ diagonal matrix, which in its diagonal has the\n",
        "image $\\beta_{ij}$. The rest of the notation is as in the previous assignment,\n",
        "$\\mathcal X= \\mathbb R^{MN}$ denotes the space of gray-scale images, and\n",
        "$\\mathcal Y = \\mathbb R^{2MN}$ denotes the space of vector-valued images.\n",
        "$\\langle\\cdot,\\cdot\\rangle_{\\mathcal X}$ is the scalar product in $\\mathbb\n",
        "R^{NM}$ and $\\langle\\cdot,\\cdot\\rangle_{\\mathcal Y}$ is the scalar product in\n",
        "$\\mathbb R^{2NM}$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kciz1U0-3kYx"
      },
      "source": [
        "**Conjugate gradient set up**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIbC5xvmzQk9"
      },
      "source": [
        "We have expressed our editing problem as a quadratic energy, however we still need a bit of work to do if we want to apply our conjugated gradient code. We need to rewrite $E$ in a more familiar way:\n",
        "\n",
        "$$E(u) = \\frac12\\langle u,\\textbf A u\\rangle - \\langle b, u\\rangle + c,$$\n",
        "\n",
        "where $c\\in\\mathbb R$ is a constant, $b\\in\\mathcal X$ and $\\textbf A$ is a $MN\\times MN$, symmetric, positive\n",
        "definite matrix.\n",
        "\n",
        "We can do this simply by manipulating the terms of the energy $E$, as follows:\n",
        "\\begin{align*}\n",
        "E(u) =& \\dfrac{1}{2}\\langle\\nabla^+u - v, \\nabla^+u - v\\rangle_{Y} +\n",
        "\\dfrac{1}{2}\\langle B(u - u_{2}), u - u_{2}\\rangle_{X}\\\\\n",
        "% =& \\dfrac{1}{2}(\\langle\\nabla^+u, \\nabla^+u\\rangle_Y - \\langle v, \\nabla^+u\\rangle_y - \\langle \\nabla^+u ,v\\rangle_Y + \\langle v ,v\\rangle_Y)\\\\\n",
        "%& + \\dfrac{1}{2}\\left(\\langle Bu , u\\rangle_{X} -\\langle Bu, u_{2}\\rangle_{X} - \\langle Bu_{2}, u \\rangle_{X} + \\langle Bu_{2}, u_{2}\\rangle_{X}\\right)\\\\\n",
        "% =& \\dfrac{1}{2}(\\langle-\\text{div}^-\\nabla^+u, u\\rangle_X + 2\\langle \\text{div}^-v, u\\rangle_X + \\langle v ,v\\rangle_Y)\\\\\n",
        "%& + \\dfrac{1}{2}\\left(\\langle Bu , u\\rangle_{X} -2\\langle Bu_{2}, u\\rangle_{X}  + \\langle Bu_{2}, u_{2}\\rangle_{X}\\right)\\\\\n",
        "=& \\dfrac{1}{2}\\langle\\underbrace{(B-\\text{div}^-\\nabla^+)}_{\\textbf A}u, u\\rangle_X - \\langle \\underbrace{Bu_2 - \\text{div}^-v }_{\\textbf b}, u\\rangle_X + \\underbrace{\\dfrac12 \\langle v ,v\\rangle_Y\n",
        " + \\dfrac12\\langle Bu_{2}, u_{2}\\rangle_{X}}_{c}\n",
        "\\end{align*}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9tLMehNIQTvB"
      },
      "source": [
        "We have now identified the matrix $\\textbf A$ and vector $\\mathbf b$ of our quadratic energy. To minimize the energy using the conjugate gradient the only thing left is implementing the callback function that computes $\\mathbf Au$ and computing the vector $\\mathbf b$.\n",
        "\n",
        "We will need the `im_bwd_divergence` function from previous assignments that computes the action of $\\text{div}^-$. Recall that $\\text{div}^- = - (\\nabla^+)^T$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "alOrnveK3kYy"
      },
      "outputs": [],
      "source": [
        "def im_bwd_divergence(vector_image_i: np.ndarray,\n",
        "                      vector_image_j: np.ndarray):\n",
        "    \"\"\"\n",
        "    Discrete divergence of a vector image using backward differences.\n",
        "    This is the negative transpose of the im_fwd_gradient\n",
        "\n",
        "    :param vector_image_i: MxN image of vectical vector components\n",
        "    :param vector_image_j: MxN image of horizontal vector components\n",
        "\n",
        "    :return divg: backwards divergence of vector_image\n",
        "    \"\"\"\n",
        "    divg = None ### WRITE YOUR SOLUTION\n",
        "\n",
        "    # TODO: Backwards i partial derivative of gradient_i\n",
        "\n",
        "\n",
        "    # TODO: Backwards j partial derivative of gradient_j\n",
        "\n",
        "    return divg"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73NjqgFvZg3X"
      },
      "source": [
        "<font color='blue'>**(QUESTION\n",
        "10)** Complete the following code cell. Given an image $u$ and a mask $\\beta$, the function `poisson_linear_operator` receives $u$ and $\\beta$ as input and returns\n",
        "$$\\textbf Au = (B - \\text{div}^-\\nabla^+)u = \\beta u - \\text{div}^-\\nabla^+u.$$</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nCE_Jvh73kYz"
      },
      "outputs": [],
      "source": [
        "def poisson_linear_operator(u: np.array, beta: np.array):\n",
        "    \"\"\"\n",
        "    Implements the action of the matrix A in the quadratic energy associated\n",
        "    to the Poisson editing problem.\n",
        "\n",
        "    :param u: an image (MxN)\n",
        "    :param beta: the mask image (MxN)\n",
        "\n",
        "    :return Au: matrix A applied to image\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO complete function\n",
        "\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRt_ccKssCVO"
      },
      "source": [
        "<font color='blue'>**(QUESTION\n",
        "11)** The following function puts everything together. Complete it by adding the computation of the vector $\\textbf b$.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5ZlKeFl3kY4"
      },
      "outputs": [],
      "source": [
        "def poisson_blending(u1: np.array,\n",
        "                     u2: np.array,\n",
        "                     mask: np.array):\n",
        "    \"\"\"\n",
        "    Uses Poisson editing to blend a region of image 1 into image 2.\n",
        "    The region is determined as the pixels where the binary mask is 1.\n",
        "\n",
        "    :param u1: grayscale image 1 (MxN)\n",
        "    :param u2: grayscale image 2 (MxN)\n",
        "    :param mask: binary image 2 (MxN)\n",
        "\n",
        "    :return upsn: Poisson blended image (MxN)\n",
        "    :return fs: array with the Poisson blending energy for each iteration\n",
        "    \"\"\"\n",
        "\n",
        "    # Define beta_0 and calculate beta\n",
        "    beta_0 = 20                           # TRY CHANGING\n",
        "    beta = beta_0 * (1 - mask)\n",
        "\n",
        "    # Calculate the right hand term with boundary data\n",
        "    vi, vj = None, None # TODO: GUIDING VECTOR FIELD\n",
        "    b = (-1) * im_bwd_divergence(vi, vj) + beta * u2\n",
        "\n",
        "    # Define the parameters to be used when calculating the conjugate gradient\n",
        "    initial_condition = np.zeros_like(u1)  # TRY CHANGING\n",
        "    tolerance = 1                          # TRY CHANGING\n",
        "    max_iters = 1000                       # TRY CHANGING\n",
        "    upsn, fs = conjugate_gradient(callback = poisson_linear_operator,\n",
        "                                  b = b,\n",
        "                                  callback_params = {\"beta\": beta},\n",
        "                                  initial_condition = initial_condition,\n",
        "                                  tolerance = tolerance,\n",
        "                                  max_iters = max_iters)\n",
        "\n",
        "    return upsn, fs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XEtAQPOZswYu"
      },
      "source": [
        "<font color='blue'>**(QUESTION 12)** Run the Poisson blending code changing different parameters (see the \"TRY CHANGING\" comments in the code).\n",
        "1. Try using different initial conditions (without changing `tolerance` and `max_iters`. Does the final result depend on the initial value?\n",
        "1. Now let us set `max_iters = 10` and try a couple of different initial conditions`. Does now the final result depend on the initial value? Are 10 iterations enough for the algorithm to converge?\n",
        "1. What is the impact of the tolerance? What happens if the tolerance is too large? What if it is too small?\n",
        "</font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3O-luDdj97y5"
      },
      "source": [
        "<font color='red'>**ANSWER**</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oeo4RvA3kY6"
      },
      "outputs": [],
      "source": [
        "ucomp = composite_gray_levels(u1, u2, mask)\n",
        "upsn, fs = poisson_blending(u1, u2, mask)\n",
        "\n",
        "# Plot\n",
        "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows = 2, ncols = 2, figsize = (16, 8))\n",
        "ax1.imshow(u1, cmap = \"gray\")\n",
        "ax1.set_title(\"Ginevra\")\n",
        "ax2.imshow(u2, cmap = \"gray\")\n",
        "ax2.set_title(\"Lisa\")\n",
        "ax3.imshow(upsn, cmap = \"gray\")\n",
        "ax3.set_title(\"composing gradients\")\n",
        "ax4.imshow(ucomp, cmap = \"gray\")\n",
        "ax4.set_title(\"composing gray levels\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RfV8zbwatMlk"
      },
      "source": [
        "<font color='blue'>**(QUESTION\n",
        "13)** Take a picture of two different faces (u1 and u2) and apply Poisson editing (it will be better if the faces are from the two components of the group). Try using u1 as a background and the face of u2 and the way around. Notice that you will need to create your own mask.</font>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FIq5cObxKm3U"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
