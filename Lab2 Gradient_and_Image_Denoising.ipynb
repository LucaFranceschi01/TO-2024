{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"JM_ErGq-k_3c"},"source":["# Group Members:"]},{"cell_type":"markdown","source":["<font color='blue'>**PUT NAMES OF YOUR TEAM MEMBERS HERE**"],"metadata":{"id":"kabcI8Wj5cKx"}},{"cell_type":"markdown","source":["*   Name 1\n","*   Name 2\n","*   Name 3"],"metadata":{"id":"Fnul20Qf5f0s"}},{"cell_type":"markdown","source":["# Guide"],"metadata":{"id":"oJoeM0yp_REU"}},{"cell_type":"markdown","source":["In this practice, we will cover the following topics:\n","\n","* Gradient Descent and\n","* Image Denoising.\n","\n","First, we will study the *gradient descent* algorithm, one of the\n","simplest (and more general) function minimization methods. We will consider a toy problem: the minimization of a function $f:\\mathbb R^2\\rightarrow \\mathbb\n","R$. The second part of the practice is on the application of the gradient\n","descent method to remove the noise in an image, via the minimization of a\n","denoising energy.\n","\n","\\\\\n","\n","For any doubts before and after the practice, you can contact your teacher:\n","\n","Nneka Okolo - nnekamaureen.okolo@upf.edu\n","\n","Pablo Arias - pablo.arias@upf.edu\n","\n","Adriano Pastore - adriano.pastore@upf.edu\n","\n","\\\\\n","\n","**Pre-requisites:** Before the practice, you should review the following topics:\n","\n","*   Gradient of a function $f:\\mathbb R^n\\rightarrow \\mathbb R$.\n","*   Level sets (or level lines) of a function $f:\\mathbb R^n\\rightarrow \\mathbb R$, and its geometrical relation to the gradient.\n","*   Directional derivatives, and how to compute them using the gradient.\n","\n","\\\\\n","\n","**Deadlines**: See\n","[P101](https://calendar.google.com/calendar/embed?src=c_b679939a9db8a1d8cd9f01f62d373d173f76794e4137c40e793a8d2cb11708f8%40group.calendar.google.com&ctz=Europe%2FMadrid/),\n","[P102](https://calendar.google.com/calendar/embed?src=c_5a65338fe8c3ce7909e62bb6b572b1a61ff4ad3543b12f72468e1a16bca41bd0%40group.calendar.google.com&ctz=Europe%2FMadrid),\n","[P201](https://calendar.google.com/calendar/embed?src=c_58aa336a0c5d0a38b13dd4a38071e7d8f9a18f4306ffeef2e48276087c339163%40group.calendar.google.com&ctz=Europe%2FMadrid),\n","[P202](https://calendar.google.com/calendar/embed?src=c_dac1d492e1060f3cee35420a9c2ff0d345e89a002cc8c70fe74bf0b78bf99d37%40group.calendar.google.com&ctz=Europe%2FMadrid),\n","\n","\\\\\n","\n","**Submission instructions**\n","\n","Register your group members [here](https://forms.gle/NLeYqhN6LyPnSPg78) if you haven't already.\n","\n","Complete the code and answer the questions below.\n","\n","Export the notebook with the answers using the menu option File->Download .ipynb.\n","\n","Rename exported notebook with the format **lastnameUid.ipynb** where lastname is the first surname of **Member 1** in the form and Uid is their UPF ID.\n","\n","Submit your solution [here](https://forms.gle/AdYQwDEjAta1QaRY6) by the deadline. **Only one member needs to complete this step**.\n","\n","You will receive an acknowledgement of receipt.\n","\n","\\\\\n","\n","**Grading**:\n","\n","  The evaluation is based on results, conclusions and the commented code together.\n","\n","\n","\n","[comment]: <> (Macros:)\n","$\\newcommand{\\m}{-}\n","\\newcommand{\\ma}[1]{\\boldsymbol{#1}}\n","\\newcommand{\\tras}[1]{#1^{\\mathrm{T}}}\n","\\newcommand{\\herm}[1]{#1^{\\mathrm{H}}}\n","\\newcommand{\\con}[1]{#1^{\\mathrm{*}}}\n","\\newcommand{\\E}{\\mathbb{E}}\n","\\newcommand{\\tech}[1]{\\overline{#1}}\n","\\newcommand{\\nspace}{\\!\\!\\!\\!}\n","\\newcommand{\\nmbr}[1]{\\oldstylenums{#1}}\n","\\newcommand{\\eg}{\\emph{e.g}. } \\newcommand{\\Eg}{\\emph{E.g}. }\n","\\newcommand{\\ie}{\\emph{i.e}. } \\newcommand{\\Ie}{\\emph{I.e}. }\n","\\newcommand{\\cf}{\\emph{c.f}. } \\newcommand{\\Cf}{\\emph{C.f}. }\n","\\newcommand{\\etc}{\\emph{etc}. } \\newcommand{\\vs}{\\emph{vs}. }\n","\\newcommand{\\wrt}{w.r.t\\onedot } \\newcommand{\\dof}{d.o.f. }\n","\\newcommand{\\etal}{\\emph{et al}. }\n","\\newcommand{\\R}{\\mathbb{R}}\n","\\newcommand{\\sign}{\\mathrm{sign}}\n","\\newcommand{\\eps}{\\varepsilon}\n","\\newcommand{\\To}{\\longrightarrow}\n","\\DeclareMathOperator*{\\argmin}{arg\\,min}\n","\\DeclareMathOperator*{\\argmax}{arg\\,max}$"],"metadata":{"id":"JCfOu0TE55xS"}},{"cell_type":"markdown","source":["**Instructions for answering the questions.**\n","\n","Questions are indicated in blue. Some questions require answers in the form of text, some others require completing code. See the examples below. *Please do not modify the notebook outside of these cells.*"],"metadata":{"id":"jzaSqp7hJYC-"}},{"cell_type":"markdown","source":["<font color='blue'>**(QUESTION 42)** Based on what you know at this moment, answer these questions:\n","1. What are your favorite subjects?\n","2. What are your favourite hobbies?\n","</font>"],"metadata":{"id":"R-IHsJZ0JaKX"}},{"cell_type":"markdown","source":["<font color='red'>**ANSWER**</font>\n","\n","1. I only like one subject: \"Optimization Techniques.\"\n","1. I like writing equations $e^{i\\pi} + 1 = 0$"],"metadata":{"id":"xGxO_cnuJcRs"}},{"cell_type":"markdown","source":["<font color='blue'>**(QUESTION 43)** This is a coding question. There is no <font color='red'>**ANSWER**</font> cell. Instead, you should complete the code cell following the question. Typically, you'll find TODOs in the code indicating the places that you are expected to complete.\n","</font>"],"metadata":{"id":"LjfQhH4OJeoc"}},{"cell_type":"code","source":["a = None     # TODO substitute the None by a nice number to print\n","print(\"The number a is {}\".format(a))"],"metadata":{"id":"P1YokEN8JlSb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ndG1DGqaGlex"},"source":["# Part 1: Gradient Descent"]},{"cell_type":"code","source":["# import required libraries\n","import numpy as np\n","from PIL import Image\n","import matplotlib.pyplot as plt\n","import os\n","\n","from mpl_toolkits.mplot3d import Axes3D\n","from matplotlib import cm\n","from IPython import display"],"metadata":{"id":"RQHTKl007aGg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Minimization of a toy function"],"metadata":{"id":"zKwqfkE071-_"}},{"cell_type":"markdown","source":["We will use the gradient descent method to compute the minima of the function\n","$f:\\mathbb R^2\\rightarrow \\mathbb R$ given by,\n","\n","$$f(x_1,x_2) = \\frac1{1000}\\left(x_1^4 + x_2^4 - 80 x_1^2 - 60 x_2^2 + 100x_1 +\n","50 x_2 + 1\\right)$$"],"metadata":{"id":"p2oY5guR81Qt"}},{"cell_type":"markdown","source":["<font color='blue'>**(QUESTION 1.1)**  Complete the Python functions **toy_fun** and **toy_gradient**. These functions implement the function $f$ and its gradient. Follow the comments provided in the code."],"metadata":{"id":"qLFteyEk9G1T"}},{"cell_type":"code","metadata":{"id":"iu4TQq6cGlez"},"source":["def toy_fun(x: np.ndarray):\n","    \"\"\"\n","    Polynomial toy function - see the guide\n","\n","    :param x: array [x_1, x_2]\n","\n","    :return y: value of the function at point x\n","    \"\"\"\n","    # TODO: Compute the function\n","    y = None ### WRITE YOUR SOLUTION\n","    return y"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["toy_fun(np.array([100,2]))"],"metadata":{"id":"RYfobzAISRUf"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DmqASbCOGle0"},"source":["def toy_gradient(x: np.ndarray):\n","    \"\"\"\n","    Gradient of toy_fun polynomial toy function\n","\n","    :param x: 2x1 array\n","    :return grad: 2x1 array: gradient of the toy function at point x\n","    \"\"\"\n","    # TODO: Compute the gradient of the toy function (must be calculated by hand)\n","    grad = None ### WRITE YOUR SOLUTION\n","    return grad"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["x = np.array([[1,2]]).T\n","toy_gradient(x)"],"metadata":{"id":"JzT5LlzgSVpl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nTGNASsYGle1"},"source":["<font color='blue'>**(QUESTION 1.2)** Complete the Python function **gradient_descent**.\n","This function implements a gradient descent algorithm. We are going to\n","implement it in a way in which we can use the same gradient descent function\n","for this toy example and for the denoising energy of the next section. Follow the comments provided in the code."]},{"cell_type":"code","metadata":{"id":"I1ZgrgcRGle1"},"source":["def gradient_descent(callback,\n","                     callback_params: dict,\n","                     initial_condition: np.ndarray,\n","                     step_size: float,\n","                     max_iterations: int,\n","                     tolerance: float,\n","                     fig = None,\n","                     ax = None):\n","    \"\"\"\n","    Implementation of the gradient descent algorithm with\n","    fixed step size. It uses function handles (handles are MatLab pointers). It\n","    can work with any function and gradient if they are implemented with . Here params\n","    is a structure with the internal parameters of my_fun and my_grad.\n","\n","    :param callback: gradient of function to be optimized\n","    :param callback_params: a structure with the internal parameters of the target function and\n","                            its gradient. Useful for the image denoising task.\n","    :param initial_condition: initial condition for gradient descent\n","    :param step_size: size of the gradient descent steps\n","    :param max_iterations: maximum number of iterations\n","    :param tolerance: tolerance for the stopping condition (it stop when\n","                      the norm of the gradient is below the tolerance)\n","\n","    :return current_value: value found\n","    \"\"\"\n","    # Initialize variables\n","    current_value = initial_condition\n","    previous_value = current_value\n","    current_iteration = 0\n","    current_norm_value = np.inf\n","    iterates = []\n","\n","    # Main loop for Gradient Descent\n","    while (current_norm_value > tolerance) and (current_iteration < max_iterations):\n","        # Keep previous - just for visualization\n","        previous_value = current_value\n","\n","        # TODO: Run the gradient descent\n","        gf = None ### WRITE YOUR SOLUTION\n","\n","        # TODO: Update the current value and norm value of gradient\n","        current_value = None ### WRITE YOUR SOLUTION\n","        current_norm_value = None ### WRITE YOUR SOLUTION\n","        print(\"{} of {} -> tolerance: {}\".format(current_iteration, max_iterations, current_norm_value))\n","\n","        # Plot current position! Just for visualization purposes\n","        # if x is a 2x1 vector (visualization of toy example)\n","        if (current_value.shape[0] == 2) & (current_value.shape[1] == 1):\n","            if not ax:\n","                fig, ax = plt.subplots()\n","            ax.plot(current_value[1, 0], current_value[0, 0], marker = 'o', color = \"k\")\n","            ax.plot([previous_value[1, 0], current_value[1, 0]],\n","                    [previous_value[0, 0], current_value[0, 0]], \"-k\")\n","            display.clear_output(wait=True)\n","            display.display(fig)\n","\n","        # Update the iteration\n","        current_iteration += 1\n","        # Collect iterates\n","        iterates.append(current_value.copy())\n","\n","    return current_value, iterates"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fYVlFqz2Gle3"},"source":["<font color='blue'>**(QUESTION 1.3)**   Run the function **toy_main** with several time steps and several initial conditions. What do you observe?"]},{"cell_type":"code","metadata":{"id":"wCyMh-1jGle3"},"source":["def toy_main(initial_condition: np.ndarray,\n","             step_size: float,\n","             max_iterations: int):\n","    # Set the grids\n","    x1 = np.arange(-10, 10, 0.1)\n","    x2 = np.arange(-10, 10, 0.1)\n","\n","    # Evaluate the toy_fun\n","    y = np.zeros(shape = (len(x1), len(x2)))\n","    for i, x1_value in enumerate(x1):\n","        for j, x2_value in enumerate(x2):\n","            y[i, j] = toy_fun([x1_value, x2_value])\n","\n","    # Plot the surface.\n","    fig = plt.figure(figsize = (14, 8))\n","    # ax = fig.gca(projection='3d')\n","    ax = fig.add_subplot(projection='3d')\n","    X, Y = np.meshgrid(x1, x2)\n","    Z = y\n","    surf = ax.plot_surface(X, Y, Z, cmap=cm.coolwarm,\n","                           linewidth=0, antialiased=False)\n","\n","    # Add a color bar which maps values to colors.\n","    fig.colorbar(surf, shrink=0.5, aspect=5)\n","\n","    # Show the surface\n","    #plt.show()\n","\n","    # Plot the contours\n","    fig_contours, ax_contours = plt.subplots(figsize = (10, 8))\n","    CS = ax_contours.contour(X, Y, Z,\n","                    corner_mask = False, levels = 150,\n","                    linewidths=(1,), cmap = cm.coolwarm)\n","\n","    # Set initial condition --> TRY CHANGING IT\n","    x0 = initial_condition\n","\n","    # Set additional gradient descent parameters --> EXPLORE USING DIFFERENT PARAMETERS\n","    tolerance = 0.01\n","\n","    # Call gradient descent minimization\n","    print(\"First gradient descent ...\")\n","    xs = gradient_descent(callback = toy_gradient,\n","                          callback_params = {},\n","                          initial_condition = x0,\n","                          step_size = step_size,\n","                          max_iterations = max_iterations,\n","                          tolerance = tolerance,\n","                          fig = fig_contours,\n","                          ax = ax_contours)\n","    return xs"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r6onnRAWGle5"},"source":["init_condition = np.array([[3], [-1]])\n","step_size_ = 1\n","max_iters = 200\n","\n","xs_ = toy_main(init_condition,\n","                step_size_,\n","                max_iters)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<font color='red'>**ANSWER**"],"metadata":{"id":"qVYTnogl1x8y"}},{"cell_type":"markdown","source":["<font color='blue'>**(QUESTION 1.3a)** Look at the plots returned by **toy_main**. How many local minima does the function have in the domain\t$[-10,10]\\times [-10,10]$?"],"metadata":{"id":"mMtjdg-612vJ"}},{"cell_type":"markdown","source":["<font color='red'>**ANSWER**"],"metadata":{"id":"7C-46jRd2A3Z"}},{"cell_type":"markdown","source":["<font color='blue'>**(QUESTION 1.3b)** Run **toy_main** starting from $x^0 = [-2,-8]$. Does it converge to the global minimum?"],"metadata":{"id":"CzSeKTCa2Cjo"}},{"cell_type":"code","source":["# TODO: Set initial condition and run toy_main again\n","### WRITE YOUR SOLUTION"],"metadata":{"id":"Gms0jApI7ILO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<font color='red'>**ANSWER**"],"metadata":{"id":"97bz9Lb52Eby"}},{"cell_type":"markdown","source":["<font color='blue'>**(QUESTION 1.3c)** Plot the log error. Hint: refer to slides on \"Convergence rate of iterative methods\" from the theory lectures."],"metadata":{"id":"qsDEsSit2Crp"}},{"cell_type":"code","source":["# TODO: Get the global minima\n","### WRITE YOUR SOLUTION"],"metadata":{"id":"xcfPlY-GN5nl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: Compute the log error using the output from toy_main\n","### WRITE YOUR SOLUTION"],"metadata":{"id":"MIOGLkTA2PDJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# TODO: Plot log error against the number of time steps\n","### WRITE YOUR SOLUTION"],"metadata":{"id":"5ciHc2ZE76Ys"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<font color='blue'>**(QUESTION 1.3d)** Estimate the rate of convergence from the logarithmic plot you have created."],"metadata":{"id":"6X9F8LnT5Q9L"}},{"cell_type":"markdown","source":["<font color='red'>**ANSWER**"],"metadata":{"id":"lz2pAOHB5hti"}},{"cell_type":"markdown","source":["<font color='blue'>**(QUESTION 1.3e)** Try **toy_main** with different step sizes. Which step sizes yield a faster convergence?\tWhich are more accurate?"],"metadata":{"id":"1_pKImsy2Cxw"}},{"cell_type":"code","source":["# TODO: Run toy_main with different step sizes\n","### WRITE YOUR SOLUTION"],"metadata":{"id":"oFL8Jmxt2579"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<font color='red'>**ANSWER**"],"metadata":{"id":"KLdv2U0M2E9g"}},{"cell_type":"markdown","metadata":{"id":"v8Dik7oYGle6"},"source":["# Part 2: Image denoising energy"]},{"cell_type":"markdown","source":["In this section, we will minimize an energy which enforces close-by pixels to\n","have a similar value. To minimize such an energy, we will use the gradient-descent method implemented above. Since this is the first assignment in which we work with energies defined over images,\n","we will start by introducing some notation."],"metadata":{"id":"AhRUv3ys-ruM"}},{"cell_type":"markdown","source":["## Discrete images"],"metadata":{"id":"eNr2nCS6_CwM"}},{"cell_type":"markdown","source":["We consider two types of images:\n","*Scalar images* and *vector-valued images*.  A scalar image represents an image\n","with one channel, typically a gray-scale image. Vector-valued images are images\n","which for each pixel has a vector. It could be for example an RGB image: at\n","each pixel, we have a 3-dimensional vector with the R, G and B components.\n","As we are going to see next, although we will not work with color images, we\n","still need vector valued images for gradient images."],"metadata":{"id":"xaW0AG68_wzh"}},{"cell_type":"markdown","source":["### Scalar images"],"metadata":{"id":"FbmTdDCO_yv5"}},{"cell_type":"markdown","source":["We define a scalar discrete image as a real function\n","$u:\\Omega\\rightarrow \\mathbb R$  defined over the rectangular discrete lattice\n","$\\Omega = \\{1,\\dots,M\\}\\times\\{1,\\dots,N\\}$ ($N$ columns and $M$ rows). We\n","refer to the image value at location $(i,j)\\in\\Omega$ (row $i$, column $j$) as\n","$u_{i,j}$ or $u_{ij}$. The following for example is an image defined on $\\Omega\n","= \\{1,2,3,4\\}\\times \\{1,2,3,4,5\\}$.\n","\n","\\\n","\n","$$\n","u = \\left[\n","\\begin{array}{c c c c c c c}\n","\tu_{11} & u_{12} & u_{13} & u_{14} & u_{15} \\\\\n","\tu_{21} & u_{22} & u_{23} & u_{24} & u_{25} \\\\\n","\tu_{31} & u_{32} & u_{33} & u_{34} & u_{35} \\\\\n","\tu_{41} & u_{42} & u_{43} & u_{44} & u_{45} \\\\\n","\\end{array}\n","\\right]\n","$$\n","\n","\\\n","\n","We will consider the image as a vector in  $\\mathbb R^{MN}$, by concatenating\n","the columns in a huge (column) vector:\n","\n","\\\n","\n","$$u = [\\overbrace{u_{11}, \\dots, u_{M1}}^{\\text{column 1}},\n","\\overbrace{u_{12}, \\dots, u_{M2}}^{\\text{column 2}},\n","\\quad\\cdots\\quad,\\overbrace{u_{1N},\\dots, u_{MN} }^{\\text{column N}}]^T.$$\n","\n","\\"],"metadata":{"id":"Y8tI34jw_9Lh"}},{"cell_type":"markdown","source":["We define the notation $\\mathcal X = \\mathbb R^{MN}.$ We can think of $\\mathcal X$ is the space of\n","all $M\\times N$ images. Since we consider images as vectors in $\\mathcal X$, we have a\n","scalar product and a corresponding norm given as follows:\n","\n","\n","$$\\left\\langle u,v \\right\\rangle_\\mathcal X = \\sum_{i = 1}^M\\sum_{j = 1}^N\n","u_{ij}v_{ij}\\quad\\text{and}\\quad \\|u\\|_{\\mathcal X} = \\sqrt{\\langle u,u\n","\\rangle_\\mathcal X} = \\sqrt{ \\sum_{i = 1}^M\\sum_{j = 1}^N u_{ij}^2},$$\n","\n","where $u,v\\in\\mathcal X$ are two images. In some cases we will call these the\n","$\\mathcal X$-scalar product and the $\\mathcal X$-norm."],"metadata":{"id":"AFIoig27BUE-"}},{"cell_type":"markdown","source":["<font color= \"red\">**Hint**: *Notice in Python the difference between matrix multiplication and element-wise multiplication when working with* \"np.ndarray\"."],"metadata":{"id":"lIURAYaHBcbA"}},{"cell_type":"markdown","source":["### Vector images"],"metadata":{"id":"BPms_WBJBJ7f"}},{"cell_type":"markdown","source":["We will also consider *vector-valued images* $g:\\Omega\\rightarrow \\mathbb R^2$.\n","The value of $g$ at pixel $i,j$ is a two component vector $g_{ij} =\n","(g_{1,ij},g_{2,ij})\\in\\mathbb R^2$.\n","The following for example is an image defined on $\\Omega\n","= \\{1,2,3,4\\}\\times \\{1,2,3,4,5\\}$:\n","\n","\\\n","\n","$$g = \\left[\n","\\begin{array}{c c c c c c c}\n","\t(g_{1,11},g_{2,11}) & (g_{1,12},g_{2,12}) & (g_{1,13},g_{2,13}) & (g_{1,14},g_{2,14}) & (g_{1,15},g_{2,15}) \\\\\n","\t(g_{1,21},g_{2,21}) & (g_{1,22},g_{2,22}) & (g_{1,23},g_{2,23}) & (g_{1,24},g_{2,24}) & (g_{1,25},g_{2,25}) \\\\\n","\t(g_{1,31},g_{2,31}) & (g_{1,32},g_{2,32}) & (g_{1,33},g_{2,33}) & (g_{1,34},g_{2,34}) & (g_{1,35},g_{2,35}) \\\\\n","\t(g_{1,41},g_{2,41}) & (g_{1,42},g_{2,42}) & (g_{1,43},g_{2,43}) & (g_{1,44},g_{2,44}) & (g_{1,45},g_{2,45})\n","\\end{array}\n","\\right].$$\n","\n","\\\n","\n","Note that vector-valued images can be separated into two components or channels\n","$g_1$ and $g_2$. Each component is a scalar image: $g_1,g_2 \\in \\mathcal X$.\n","\n","\\\n","\n","$$\n","g_1 = \\left[\n","\\begin{array}{c c c c c c c}\n","\tg_{1,11} & g_{1,12} & g_{1,13} & g_{1,14} & g_{1,15} \\\\\n","\tg_{1,21} & g_{1,22} & g_{1,23} & g_{1,24} & g_{1,25} \\\\\n","\tg_{1,31} & g_{1,32} & g_{1,33} & g_{1,34} & g_{1,35} \\\\\n","\tg_{1,41} & g_{1,42} & g_{1,43} & g_{1,44} & g_{1,45}\n","\\end{array}\n","\\right],\\,\\,\\,\n","g_2 = \\left[\n","\\begin{array}{c c c c c c c}\n","\tg_{2,11} & g_{2,12} & g_{2,13} & g_{2,14} & g_{2,15} \\\\\n","\tg_{2,21} & g_{2,22} & g_{2,23} & g_{2,24} & g_{2,25} \\\\\n","\tg_{2,31} & g_{2,32} & g_{2,33} & g_{2,34} & g_{2,35} \\\\\n","\tg_{2,41} & g_{2,42} & g_{2,43} & g_{2,44} & g_{2,45}\n","\\end{array}\n","\\right].\n","$$"],"metadata":{"id":"w8wm0DTPBLZh"}},{"cell_type":"markdown","source":["We will arrange vector-valued images into a vector in $\\mathbb R^{2MN}$ by\n","\"vectorizing\" first the first component followed by the second:\n","\n","\\\n","\n","\\begin{multline*}\n","g = [\\overbrace{g_{1,11}, \\dots, g_{1,N1}}^{\\text{col. 1 of comp.\n","1}},\\quad\\cdots\\quad \\overbrace{g_{1,1M}, \\dots, g_{1,NM}}^{\\text{col. M\n","of comp. 1}},\\\\\\overbrace{g_{2,11}, \\dots, g_{2,N1}}^{\\text{col. 1 of comp.\n","2}},\\quad\\cdots\\quad \\overbrace{g_{2,1M}, \\dots, g_{2,NM}}^{\\text{col. M\n","of comp. 2}}]^T\\in\\mathcal Y = \\mathbb R^{2MN}.\n","\\end{multline*}\n","\n","\\\n","\n","The space of all $M\\times N$ vector-valued images (with two-component vectors) will be denoted by $\\mathcal Y = \\mathbb\n","R^{2MN}.$ Note that we have to be careful to avoid confusions: the value of a vector-valued image $g$ at pixel $(i,j)$, $g_{ij}$, is a vector in $\\mathbb R^2$. On the other hand $g$ itself is a vector in $\\mathcal Y = \\mathbb R^{2MN}$. Therefore we will use different notations for the scalar product and the norm in $\\mathbb R^2$ and in $\\mathcal Y$.\n","\n","For vectors in $a = (a_1,a_2), b = (b_1,b_2)\\in\\mathbb R^2$ we use the\n","following notations:\n","\n","$$a\\cdot b = a_1b_1 + a_2b_2\\quad \\text{ and }\\quad\n","|a| = \\sqrt{a\\cdot a} = \\sqrt{a_1^2 + a_2^2}.$$\n","\n","On the other hand, for two vector-valued images $g,h$ in $\\mathcal Y$, we\n","will use the following $\\mathcal Y$-scalar product and $\\mathcal Y$-norm, which is defined based the\n","scalar product in $\\mathbb R^2$:\n","\n","$$\n","\\left\\langle g,h \\right\\rangle_\\mathcal Y = \\sum_{i = 1}^M\\sum_{j = 1}^N\n","\tg_{ij}\\cdot h_{ij}.\n","$$"],"metadata":{"id":"dJB-di39CCKR"}},{"cell_type":"markdown","source":["The $\\mathcal Y$-scalar product between two vector-valued images can be computed\n","as the sums of the $\\mathcal X$-scalar products of their channels as follows\n","\n","\\\n","\n","\\begin{multline*}\n","\t\\left\\langle g,h \\right\\rangle_\\mathcal Y = \\sum_{i = 1}^M\\sum_{j = 1}^N\n","\tg_{ij}\\cdot h_{ij} = \\sum_{i = 1}^M\\sum_{j = 1}^N (g_{1,ij}h_{1,ij} + g_{2,ij}h_{2,ij})\\\\\n","\t= \\sum_{i = 1}^M\\sum_{j = 1}^N g_{1,ij}h_{1,ij}  + \\sum_{i = 1}^M\\sum_{j =\n","\t1}^N g_{2,ij}h_{2,ij} = \\langle g_1,h_1\\rangle_{\\mathcal X} + \\langle g_2,h_2\\rangle_{\\mathcal X},\n","\\end{multline*}\n","\n","\\\n","\n","The $\\mathcal Y$-norm is defined similarly, and as we show next, it can also\n","be computed in terms of the $\\mathcal X$-norm of the channels:\n","\n","\\\n","\n","\\begin{equation*}\n","  \\|g\\|_{\\mathcal Y}\n","  = \\sqrt{\\langle g,g \\rangle_\\mathcal Y}\n","  = \\sqrt{ \\sum_{i = 1}^M\\sum_{j = 1}^N |g_{ij}|^2}\n","  = \\sqrt{\\sum_{i = 1}^M\\sum_{j = 1}^N (g_{1,ij}^2 + g_{2,ij}^2)} = \\sqrt{\\|g_1\\|_{\\mathcal X}^2 + \\|g_2\\|_{\\mathcal X}^2}.\n","\\end{equation*}"],"metadata":{"id":"Ih90sQlsCtfZ"}},{"cell_type":"markdown","source":["## The discrete gradient as a matrix"],"metadata":{"id":"6c9fd-mlDnvx"}},{"cell_type":"markdown","source":["Since we are working with discrete images, we will consider a discrete\n","approximation of the gradient, using *forward differences*. We will use\n","the notation $\\nabla^+ u$ to refer to the image forward gradient. The discrete\n","gradient of $u$ is a vector-valued image, $\\nabla^+u:\\Omega\\rightarrow \\mathbb R^2$. It has two components for the horizontal and\n","vertical partial derivatives.\n","Thus we have that\n","\n","$$\\nabla^+ u_{ij} =\n","\\left(\n","\t\\nabla^+_i u_{ij},\n","\t\\nabla^+_j u_{ij}\n","\\right)\\in \\mathbb R^2.$$\n","\n","Here $\\nabla^+_i$ and $\\nabla^+_j$ refer to the forward differences partial\n","derivatives in the direction of $i$ (rows) and $j$ (columns). These are defined as follows:\n","\n","$$\\nabla^+_i u_{ij} = \\left\\{\n","\\begin{array}{l l}\n","\tu_{i+1,j} - u_{i,j} & \\text{ if } i < M\\\\\n","\t0 & \\text{ if } i = M\n","\\end{array}\n","\\right.\\quad\\quad\n","\\nabla^+_j u_{ij} = \\left\\{\n","\\begin{array}{l l}\n","\tu_{i,j+1} - u_{i,j} & \\text{ if } j < N\\\\\n","\t0 & \\text{ if } j = N\n","\\end{array}\n","\\right.$$\n","\n","The $i$ derivative corresponds to the vertical derivative, whereas\n","the $j$ derivative to the horizontal derivative.\n","\n","We can consider $\\nabla^+_i$ and $\\nabla^+_j$ as square matrices of $MN$ rows\n","and columns. We can \"derivate\" the image by computing the product of\n","$\\nabla^+_i$ by our vector representation of the image. The result is a vector\n","representation of the $i$ partial derivative."],"metadata":{"id":"Ab2FIMh-Dueh"}},{"cell_type":"markdown","source":["For a $4\\times 5$ image they would be as follows:\n","\n","<font size=2em>\n","$$\\nabla^+_j u =\n","\\left[\n","\\begin{array}{c c c c : c c c c : c c c c : c c c c : c c c c}\n","\t\\m1&   &   &   & 1 &   &   &   &   &   &   &   &   &   &   &   &   &   &   &   \\\\\n","\t   &\\m1&   &   &   & 1 &   &   &   &   &   &   &   &   &   &   &   &   &   &   \\\\\n","\t   &   &\\m1&   &   &   & 1 &   &   &   &   &   &   &   &   &   &   &   &   &   \\\\\n","\t   &   &   &\\m1&   &   &   & 1 &   &   &   &   &   &   &   &   &   &   &   &   \\\\\\hdashline\n","\t   &   &   &   &\\m1&   &   &   & 1 &   &   &   &   &   &   &   &   &   &   &   \\\\\n","\t   &   &   &   &   &\\m1&   &   &   & 1 &   &   &   &   &   &   &   &   &   &   \\\\\n","\t   &   &   &   &   &   &\\m1&   &   &   & 1 &   &   &   &   &   &   &   &   &   \\\\\n","\t   &   &   &   &   &   &   &\\m1&   &   &   & 1 &   &   &   &   &   &   &   &   \\\\\\hdashline\n","\t   &   &   &   &   &   &   &   &\\m1&   &   &   & 1 &   &   &   &   &   &   &   \\\\\n","\t   &   &   &   &   &   &   &   &   &\\m1&   &   &   & 1 &   &   &   &   &   &   \\\\\n","\t   &   &   &   &   &   &   &   &   &   &\\m1&   &   &   & 1 &   &   &   &   &   \\\\\n","\t   &   &   &   &   &   &   &   &   &   &   &\\m1&   &   &   & 1 &   &   &   &   \\\\\\hdashline\n","\t   &   &   &   &   &   &   &   &   &   &   &   &\\m1&   &   &   & 1 &   &   &   \\\\\n","\t   &   &   &   &   &   &   &   &   &   &   &   &   &\\m1&   &   &   & 1 &   &   \\\\\n","\t   &   &   &   &   &   &   &   &   &   &   &   &   &   &\\m1&   &   &   & 1 &   \\\\\n","\t   &   &   &   &   &   &   &   &   &   &   &   &   &   &   &\\m1&   &   &   & 1 \\\\\\hdashline\n","\t   &   &   &   &   &   &   &   &   &   &   &   &   &   &   &   &\\,0\\,&   &   &   \\\\\n","\t   &   &   &   &   &   &   &   &   &   &   &   &   &   &   &   &   &\\,0\\,&   &   \\\\\n","\t   &   &   &   &   &   &   &   &   &   &   &   &   &   &   &   &   &   &\\,0\\,&   \\\\\n","\t   &   &   &   &   &   &   &   &   &   &   &   &   &   &   &   &   &   &   &\\,0\\,\n","\\end{array}\n","\\right]\\cdot\n","\\left[\n","\\begin{array}{c}\n","\tu_{11} \\\\\n","\tu_{21} \\\\\n","\tu_{31} \\\\\n","\tu_{41} \\\\ \\hdashline\n","\tu_{12} \\\\\n","\tu_{22} \\\\\n","\tu_{32} \\\\\n","\tu_{42} \\\\ \\hdashline\n","\tu_{13} \\\\\n","\tu_{23} \\\\\n","\tu_{33} \\\\\n","\tu_{43} \\\\ \\hdashline\n","\tu_{14} \\\\\n","\tu_{24} \\\\\n","\tu_{34} \\\\\n","\tu_{44} \\\\ \\hdashline\n","\tu_{15} \\\\\n","\tu_{25} \\\\\n","\tu_{35} \\\\\n","\tu_{45}\n","\\end{array}\n","\\right] =\n","\\left[\n","\\begin{array}{c}\n","\tu_{12} - u_{11} \\\\\n","\tu_{22} - u_{21} \\\\\n","\tu_{32} - u_{31} \\\\\n","\tu_{42} - u_{41} \\\\ \\hdashline\n","\tu_{13} - u_{12} \\\\\n","\tu_{23} - u_{22} \\\\\n","\tu_{33} - u_{32} \\\\\n","\tu_{43} - u_{42} \\\\ \\hdashline\n","\tu_{14} - u_{13} \\\\\n","\tu_{24} - u_{23} \\\\\n","\tu_{34} - u_{33} \\\\\n","\tu_{44} - u_{43} \\\\ \\hdashline\n","\tu_{15} - u_{14} \\\\\n","\tu_{25} - u_{24} \\\\\n","\tu_{35} - u_{34} \\\\\n","\tu_{45} - u_{44} \\\\ \\hdashline\n","\t0 \\\\\n","\t0 \\\\\n","\t0 \\\\\n","\t0\n","\\end{array}\n","\\right]$$"],"metadata":{"id":"vkXcjpkkRS-4"}},{"cell_type":"markdown","source":["The discrete gradient is a vector-valued image with two components. We\n","construct a matrix for the whole discrete gradient by concatenating the matrices\n","$\\nabla^+_i$ and $\\nabla^+_j$ in a block matrix:\n","\n","$$\\nabla^+u =\n","\\left[\n","\\begin{array}{c}\n","\t \\\\\n","\t\\,\\,\\,\\nabla^+_i \\,\\,\\,\\\\\n","\t\\\\ \\hdashline\n","\t \\\\\n","\t\\,\\,\\,\\nabla^+_j \\,\\,\\,\\\\\n","\t \\\\\n","\\end{array}\n","\\right]\\cdot u = \\left.\\left[\n","\\begin{array}{c}\n","\t \\\\\n","\t\\nabla^+_i u \\\\\n","\t \\\\ \\hdashline\n","\t  \\\\\n","\t\\nabla^+_j u  \\\\\n","\t\\color{white}{.}\n","\\end{array}\n","\\right]\\right\\}\\text{(vector with } 2MN \\text{ components)}.$$"],"metadata":{"id":"ApE-xTNyLEgj"}},{"cell_type":"markdown","source":["<font color='blue'>**(QUESTION 2.1a)** Complete the Python function \"im_fwd_gradient\". This function computes the forward gradient\n","$\\nabla^+$. Follow the comments provided in the code."],"metadata":{"id":"cFlFAHgfMt4K"}},{"cell_type":"code","source":["def im_fwd_gradient(image: np.ndarray):\n","    \"\"\"\n","    Discrete gradient of an image using forward differences, with\n","    homogeneous Neuman boundary conditions.\n","\n","    :param image: an MxN image\n","\n","    :return grad_i: partial derivative in the i direction (vertical)\n","    :return grad_j: partial derivative in the j direction (horizontal)\n","    \"\"\"\n","    # TODO: Get the size of the image\n","    image_shape = None ### WRITE YOUR SOLUTION\n","\n","    # TODO: Calculate both gradients\n","    #       Check the Neuman boundary conditions\n","    grad_i = None ### WRITE YOUR SOLUTION\n","    grad_j = None ### WRITE YOUR SOLUTION\n","    return grad_i, grad_j"],"metadata":{"id":"BIAuUpFQ0NrJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["u = np.array([[1,2,0],[-1,4,3],[3,-5,1]])\n","u"],"metadata":{"id":"oEzTz8mVu3qd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["gradu_i,gradu_j=im_fwd_gradient(u)\n","print(f\"forward row gradient: \\n{gradu_i},\\nforward column gradient: \\n{gradu_j}\")"],"metadata":{"id":"eEc2h96Lu4SV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["check_gradu_i = np.allclose(gradu_i,np.array([[-2.,  2.,  3.],[ 4., -9., -2.],[ 0.,  0.,  0.]]))\n","check_gradu_j = np.allclose(gradu_j,np.array([[ 1., -2.,  0.], [ 5., -1.,  0.], [-8.,  6.,  0.]]))\n","\n","print(\"Is implementation of 'im_fwd_gradient' correct? {}\".format(check_gradu_i and check_gradu_j))"],"metadata":{"id":"x7RZBeDWmofI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<font color='blue'>**(QUESTION 2.1b)** Complete the Python function \"im_bwd_divergence\". This function computes the backward divergence $div^{-}$. Follow the comments provided in the code."],"metadata":{"id":"_iSUHQa8Nt3l"}},{"cell_type":"markdown","metadata":{"id":"WS98wPDS1DYy"},"source":["For matrix $u$, with $u^{1} = \\nabla^{+}_{i}u_{i,j}$ and $u^{2} = \\nabla^{+}_{j}u_{i,j}$:\n","\n","$$(div^{-}\\,u) =\n","\\begin{cases}\n","  u^{1}_{i,j} - u^{1}_{i-1,j} & \\text{if }1< i<M\\\\\n","  u^{1}_{i,j} & \\text{if }i=1\\\\\n","  - u^{1}_{i-1,j} & \\text{if }i=M\n","\\end{cases}\n","+\n","\\begin{cases}\n","  u^{2}_{i,j} - u^{2}_{i,j-1} & \\text{if }1< j<N\\\\\n","  u^{2}_{i,j} & \\text{if }j=1\\\\\n","  - u^{2}_{i,j-1} & \\text{if }j=N\n","\\end{cases}\n","$$"]},{"cell_type":"code","source":["def im_bwd_divergence(vector_image_i: np.ndarray,\n","                      vector_image_j: np.ndarray):\n","    \"\"\"\n","    Discrete divergence of a vector image using backward differences.\n","    This is the negative transpose of the im_fwd_gradient\n","\n","    :param vector_image_i: MxN image of vectical vector components\n","    :param vector_image_j: MxN image of horizontal vector components\n","\n","    :return divg: backwards divergence of vector_image\n","    \"\"\"\n","    divg = None ### WRITE YOUR SOLUTION\n","\n","    # TODO: Backwards i partial derivative of gradient_i\n","\n","\n","    # TODO: Backwards j partial derivative of gradient_j\n","\n","    return divg"],"metadata":{"id":"j5TN-kS9z-WO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["divg_ = im_bwd_divergence(gradu_i,gradu_j)\n","print(f\"backward divergence: \\n{divg_}\")"],"metadata":{"id":"tef1N-7FvEqc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["check_divg = np.allclose(divg_,np.array([[ -1.,  -1.,   5.], [ 11., -17.,  -4.], [-12.,  23.,  -4.]]))\n","\n","print(\"Is implementation of 'im_bwd_divergence' correct? {}\".format(check_divg))"],"metadata":{"id":"vt5KfFOnnp05"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Qd0UNkUlGle8"},"source":["## An energy for image denoising"]},{"cell_type":"markdown","source":["We now have all the ingredients to formulate our image denoising energy. Let us consider a noisy\n","image $f:\\Omega\\rightarrow \\mathbb R$. We assume that the image $f$ is the result of contaminating a\n","clean image $u^*$ by white Gaussian noise:\n","\n","$$f_{ij} = u^*_{ij} + n_{ij}, \\quad\\text{ for } 1\\leqslant i\\leqslant M, 1\\leqslant j\\leqslant N,$$\n","\n","where $n_{ij}\\sim \\mathcal N(0,\\sigma)$.\n","\n","To denoise the image, we consider a model for noiseless images (the image\n","\\emph{prior}). There are many possibilities. In this practice we will assume\n","that noiseless images have gradients with low norm. This is one of the\n","simplest models. The image prior represents what we know about the image, *before knowing the actual image*.\n","\n","We will estimate $u^*$ by computing\n","the minimum of the following energy $E:\\mathbb R^{MN}\\rightarrow \\mathbb R$:\n","\n","\\begin{equation}\n","\tE(u) = \\overbrace{\\sum_{i = 1}^M\\sum_{j = 1}^N\n","\tc_{ij}|\\nabla^+u_{ij}|^2}^{\\text{regularization}}  +\n","\t\\beta \\overbrace{\\sum_{i = 1}^M\\sum_{j = 1}^N (u_{ij} -\n","\tf_{ij})^2,}^{\\text{data attachment}}\n","\\end{equation}\n","\n","where $c:\\Omega\\rightarrow [0,1]$ is a coefficients image which controls the\n","regularization. Recall that $|\\cdot|$ denotes the 2-norm in $\\mathbb\n","R^2$:\n","\n","$$|\\nabla^+u_{ij}|^2 = \\left(\\nabla^+_iu_{ij}\\right)^2 +\n","\\left(\\nabla^+_ju_{ij}\\right)^2.$$"],"metadata":{"id":"coYTZrBnPppe"}},{"cell_type":"markdown","source":["The energy has a regularization term and a data attachment term. Let us explain\n","them:\n","\n","\\\n","\n","**Regularization term**\n","\n","The regularization term penalizes high\n","gradients. This corresponds to our model for clean images with low discrete\n","gradients: the value at a pixel should be similar to those of its neighbors.\n","This is an oversimplistic model: clean images may have discontinuities and high\n","gradients at the edges of the objects in the image. As a result, our model for\n","denoising will blur the image edges.\n","\n","This is why we add the $c_{ij}$ coefficients, to control the penalization of\n","high gradients. The idea is to use a lower $c_{ij}$ around image edges, and a\n","higher $c_{ij}$ on regions far from the edges. We will consider $c_{ij}\\in [0,1]$.\n","\n","\\\n","\n","**Data attachment term.**\n","\n","The data attachment penalizes high\n","differences with respect to the noisy data. The parameter $\\beta > 0$ sets the\n","strength of the data attachment."],"metadata":{"id":"GU9QBMG8Q8Y7"}},{"cell_type":"markdown","source":["We can express the energy in matrix notation, using our vector\n","representation of images and discrete gradients. For that, let us define some\n","useful notation for diagonal matrices. Let us consider a given scalar image\n","$c\\in \\mathcal X$. We then define $\\text{diag(c)}$ as the\n","diagonal $MN\\times MN$ matrix which has the vectorized image $c$ in the\n","diagonal. That is:\n","\n","$$\\text{diag}(c) = \\left[\n","\\begin{array}{ccc:ccc:ccc:ccc}\n","\t c_{11} &        &        &        &        &        &        &        &        &        &        &        \\\\\n","\t        & \\ddots &        &        &        &        &        &        &        &        &        &        \\\\\n","\t        &        & c_{M1} &        &        &        &        &        &        &        &        &        \\\\\\hdashline\n","\t        &        &        & c_{12} &        &        &        &        &        &        &        &        \\\\\n","\t        &        &        &        & \\ddots &        &        &        &        &        &        &        \\\\\n","\t        &        &        &        &        & c_{M2} &        &        &        &        &        &        \\\\ \\hdashline\n","\t        &        &        &        &        &        & \\quad  &        &        &        &        &        \\\\\n","\t        &        &        &        &        &        &        & \\ddots &        &        &        &        \\\\\n","\t        &        &        &        &        &        &        &        & \\quad  &        &        &        \\\\ \\hdashline\n","\t        &        &        &        &        &        &        &        &        & c_{1N} &        &        \\\\\n","\t        &        &        &        &        &        &        &        &        &        & \\ddots &        \\\\\n","\t        &        &        &        &        &        &        &        &        &        &        & c_{MN}\\!\\!\n","\\end{array}\\right],$$\n","\n","where we have only shown the non-zero entries. Observe that the multiplication\n","of a vectorized image $u$ times a diagonal matrix $\\text{diag}(c)$, $v =\n","\\text{diag}(c)u$, then $v$ corresponds to the element-wise multiplication\n","of $c$ and $u$: thus $v_{ij} = c_{ij}u_{ij}$. In fact, this is the main utility of\n","these diagonal matrices: to be able to write the element-wise multiplication\n","between images with the matrix multiplication."],"metadata":{"id":"zWimHTZCQQ97"}},{"cell_type":"markdown","source":["Now we can define our energy for image denoising as follows:\n","\n","\\\n","\n","\\begin{eqnarray}\n","\tE(u) = \\langle C \\nabla^+ u, \\nabla^+ u\\rangle_{\\mathcal Y} + \\beta \\|u\n","\t- f\\|_{\\mathcal X}^2.\n","\\end{eqnarray}\n","\n","\\\n","\n","Here $C$ is a $2MN\\times 2MN$ diagonal matrix defined as follows:\n","\n","$$C = \\left[\n","\\begin{array}{c:c}\n","\t\\text{diag}(c) &\\\\\\hdashline\n","\t& \\text{diag}(c)\n","\\end{array}\n","\\right],$$\n","\n","which in its diagonal has two copies of the vectorized image $c$. One copy\n","performs the element-wise multiplication of $c$ with the $i$ partial\n","derivative and the other with the $j$ partial derivative."],"metadata":{"id":"n9fqpK1GWBgc"}},{"cell_type":"markdown","source":["<font color='blue'>**(QUESTION 2.2a)** Complete the Python function \"denoise_energy\" following the comments provided in the\n","code."],"metadata":{"id":"i_WWpepaWzjc"}},{"cell_type":"code","metadata":{"id":"Z5Pv36snGle8"},"source":["def denoise_energy(image: np.ndarray,\n","                   noise: np.ndarray,\n","                   coefficients: np.ndarray,\n","                   beta: float):\n","    \"\"\"\n","    Evaluates the denoising energy from an image and the noisy\n","    data (see the guide)\n","\n","    :param  image: target image (MxN)\n","    :param  noise: (MxN) noisy data for attachment term\n","    :param  coefficients: (MxN) coefficients image for regularization term\n","    :param  beta: (1x1) weight of attachment term\n","\n","    :return e: energy value\n","    \"\"\"\n","\n","    # TODO: Calculate the gradient\n","    gu_i, gu_j = None, None ### WRITE YOUR SOLUTION\n","\n","    # TODO: Calculate the regularization term\n","    reg_term = None ### WRITE YOUR SOLUTION\n","\n","    # TODO: Calculate the data attachment term\n","    attach_term = None ### WRITE YOUR SOLUTION\n","\n","    # Putting everything together\n","    energy = reg_term + attach_term\n","\n","    return energy"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image_real = np.array([[1,2,3], [5,6,7], [8,9,10]])\n","image_noisy = image_real + 5 * np.ones_like(image_real)\n","res = denoise_energy(image_real,\n","                    image_noisy,\n","                    np.ones_like(image_noisy),\n","                    .05)"],"metadata":{"id":"IRwlgpHBus2t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Is implementation of 'denoise_energy' correct? {}\".format(res == 92.25))"],"metadata":{"id":"WkA12bOLs0pR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<font color='blue'>**(QUESTION 2.2b)** Compute the gradient of the denoise energy function\n","$E(u) = \\langle C \\nabla^+ u, \\nabla^+ u\\rangle_{\\mathcal Y} + \\beta \\|u\t- f\\|_{\\mathcal X}^2$."],"metadata":{"id":"awVfCZcUXS-9"}},{"cell_type":"markdown","source":["<font color='red'>**ANSWER**\n","\n","$\\nabla E(u) = ?$"],"metadata":{"id":"LOshTH0SXj6R"}},{"cell_type":"markdown","source":["<font color='blue'>**(QUESTION 2.2c)** Complete the Python function \"denoise_energy_gradient\" following the comments provided in the\n","code."],"metadata":{"id":"a4viEUjeXI6e"}},{"cell_type":"code","metadata":{"id":"NV45ms7uGle9"},"source":["def denoise_energy_gradient(image: np.ndarray,\n","                            noise: np.ndarray,\n","                            coefficients: np.ndarray,\n","                            beta: float):\n","    \"\"\"\n","    Evaluates the denoising energy gradient from an image and the noisy data (see the guide)\n","\n","    :param image: target image (MxN)\n","    :param noise: (MxN) noisy data for attachment term\n","    :param coefficients: (MxN) coefficients image for regularization term\n","    :param beta: (1x1) weight of attachment term\n","\n","    :return grade : (MxN) gradient of energy at u\n","    \"\"\"\n","    # TODO: Calculate the gradient\n","    gu_i, gu_j = None, None ### WRITE YOUR SOLUTION\n","\n","    # TODO: Calculate the gradient of data attachment term\n","    grad_reg_term = None ### WRITE YOUR SOLUTION\n","\n","    # TODO: Calculate the gradient of data attachment term\n","    grad_attach_term = None ### WRITE YOUR SOLUTION\n","\n","    # Putting everything together\n","    grade = grad_reg_term + grad_attach_term\n","\n","    return grade"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["image_real = np.array([[1,2,3], [5,6,7], [8,9,10]])\n","image_noisy = image_real + 5 * np.ones_like(image_real)\n","grade_ = denoise_energy_gradient(image_real,\n","               image_noisy,\n","               np.ones_like(image_noisy),\n","               .05)"],"metadata":{"id":"WRnZPdnt0QT8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["check_grade = np.allclose(grade_,np.array([[-10.5,  -8.5,  -6.5], [ -0.5,   1.5,   3.5], [  3.5,   5.5,   7.5]]))\n","\n","print(\"Is implementation of 'denoise_energy_gradient' correct? {}\".format(check_grade))"],"metadata":{"id":"4dEh_9VqubMD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Example with a noisy image"],"metadata":{"id":"Wm4EZZz-Ycmb"}},{"cell_type":"markdown","source":["First we need some helper files:"],"metadata":{"id":"jasvpFBbIuOj"}},{"cell_type":"code","source":["# Mount Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GC4ebz8SIvtR","executionInfo":{"status":"ok","timestamp":1710523429873,"user_tz":-60,"elapsed":26927,"user":{"displayName":"NNEKA MAUREEN OKOLO","userId":"04264045196225705265"}},"outputId":"11c0e1bd-9ee9-4ab6-c894-55a7419cfd4d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["# Move to your drive\n","%cd /content/drive/MyDrive/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wttGjBh0IzVK","executionInfo":{"status":"ok","timestamp":1710523429874,"user_tz":-60,"elapsed":6,"user":{"displayName":"NNEKA MAUREEN OKOLO","userId":"04264045196225705265"}},"outputId":"ca79121b-78c4-407a-97ae-d31c00c3c34a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive\n"]}]},{"cell_type":"code","source":["# Clone repo with auxilary files required for this and the following labs.\n","# This needs to be done only once for the course.\n","# If there is an error, then you attended the first lab! :)\n","!git clone https://github.com/Muchay/OptTechCourse_Aux.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RXtmB6gwI2wD","executionInfo":{"status":"ok","timestamp":1710523654922,"user_tz":-60,"elapsed":3903,"user":{"displayName":"NNEKA MAUREEN OKOLO","userId":"04264045196225705265"}},"outputId":"b47e9aa7-ccdb-4f20-e844-24ec28581754"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'OptTechCourse_Aux'...\n","remote: Enumerating objects: 175, done.\u001b[K\n","remote: Counting objects: 100% (175/175), done.\u001b[K\n","remote: Compressing objects: 100% (171/171), done.\u001b[K\n","remote: Total 175 (delta 0), reused 172 (delta 0), pack-reused 0\u001b[K\n","Receiving objects: 100% (175/175), 2.84 MiB | 7.05 MiB/s, done.\n","Updating files: 100% (169/169), done.\n"]}]},{"cell_type":"code","source":["# Create source path\n","source = \"/content/drive/MyDrive/OptTechCourse_Aux/Lab2/\""],"metadata":{"id":"nDEuiGyGI7iS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we create a noisy image and attempt to denoise it."],"metadata":{"id":"CyX1JpCfaN8b"}},{"cell_type":"markdown","metadata":{"id":"gNKaD7BLGle-"},"source":["**Load the image**"]},{"cell_type":"code","metadata":{"id":"-G8gLw-KGle-"},"source":["images_dir = os.path.abspath(source+\"images\")\n","image_real = np.array(Image.open(os.path.join(images_dir, \"lena.pgm\")))\n","plt.figure()\n","plt.title(\"Real image\")\n","plt.imshow(image_real, cmap = \"gray\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AY0MXxjRGle-"},"source":["**Add noise to the image**"]},{"cell_type":"code","metadata":{"id":"scxpC3UCGle-"},"source":["image_noisy = image_real + np.random.uniform(high = 50, size = image_real.shape)\n","plt.figure()\n","plt.title(\"Noisy data\")\n","plt.imshow(image_noisy, cmap = \"gray\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gJXlcTixGle_"},"source":["**Run the Gradient Descent**"]},{"cell_type":"code","metadata":{"id":"_oq7ub_4Gle_"},"source":["# Define the gradient descent parameters\n","callback_params = {\n","    \"noise\": image_noisy,\n","    \"coefficients\": np.ones_like(image_noisy),    # --> CHANGE THIS AND COMPARE\n","    \"beta\": .05                                   # --> CHANGE THIS AND COMPARE\n","}\n","step_size = .01                                   # --> CHANGE THIS AND COMPARE\n","max_iterations = 200                              # --> CHANGE THIS AND COMPARE\n","tolerance = .1                                    # --> CHANGE THIS AND COMPARE\n","\n","# Run the gradient descent\n","image_gd, _ = gradient_descent(callback = denoise_energy_gradient,\n","                            callback_params = callback_params,\n","                            initial_condition = image_noisy,\n","                            step_size = step_size,\n","                            max_iterations = max_iterations,\n","                            tolerance = tolerance)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7N8ReyIaGle_"},"source":["**Show the results**"]},{"cell_type":"code","metadata":{"id":"c0fnCC2-GlfA"},"source":["# Show the different images\n","fig, (ax1, ax2, ax3) = plt.subplots(nrows = 1, ncols = 3, figsize = (20, 12))\n","ax1.imshow(image_real, cmap = \"gray\")\n","ax1.set_title(\"Real image\")\n","ax2.imshow(image_noisy, cmap = \"gray\")\n","ax2.set_title(\"Noisy data\")\n","ax3.imshow(image_gd, cmap = \"gray\")\n","ax3.set_title(\"Gradient Descent image\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q4Jk4yFTGlfA"},"source":["# Show the absolute error among images\n","noise = abs(image_real - image_noisy)\n","denoised = abs(image_gd - image_real)\n","\n","# Plot\n","fig, (ax1, ax2) = plt.subplots(nrows = 1, ncols = 2, figsize = (20, 12))\n","ax1.imshow(noise, cmap = \"gray\")\n","ax1.set_title(\"Absolute error: real image vs noisy\")\n","ax2.imshow(denoised, cmap = \"gray\")\n","ax2.set_title(\"Absolute error: real image vs denoised\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z99GpI5LGlfA"},"source":["plt.figure(figsize = (20, 12))\n","plt.imshow(abs(image_gd - image_noisy), cmap = \"gray\")\n","plt.title('method noise')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<font color='blue'>**(QUESTION 2.3a)** Run the example with different\n","parameters (for instance, changing the maximum number of iterations, $c$, $\\beta$,etc. Can you explain what you observe?"],"metadata":{"id":"UO_WHa9wYx2z"}},{"cell_type":"markdown","source":["<font color='red'>**ANSWER**"],"metadata":{"id":"vL6bK2sIZWcy"}},{"cell_type":"markdown","source":["<font color='blue'>**(QUESTION 2.3b)** What are the effects of changing $c$?"],"metadata":{"id":"7qArnuSOZRHP"}},{"cell_type":"markdown","source":["<font color='red'>**ANSWER**"],"metadata":{"id":"XhONtAYbZ0gW"}},{"cell_type":"markdown","source":["<font color='blue'>**(QUESTION 2.3c)** Try to find the parameters gives you a smaller Mean Square Error. You can use the Python function $mse$."],"metadata":{"id":"NhrxyVN7Ze2m"}},{"cell_type":"code","source":["# TODO: Run example with different parameters and check MSE\n","### WRITE YOUR SOLUTION"],"metadata":{"id":"bf_UbPfSZiA3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["<font color='red'>**ANSWER**"],"metadata":{"id":"Pmr-Vxl5Z1LA"}}]}