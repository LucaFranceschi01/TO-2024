{"cells":[{"cell_type":"markdown","metadata":{"id":"a948U3waVysn"},"source":["# Group Members:\n","\n"]},{"cell_type":"markdown","metadata":{"id":"g0zygXFpoX88"},"source":["<font color='blue'>**PUT NAMES OF YOUR TEAM MEMBERS HERE**"]},{"cell_type":"markdown","metadata":{"id":"yHAz4s_noa7b"},"source":["*   Luca Franceschi - u99149\n","*   Jan Corcho - uXXXXXX\n"]},{"cell_type":"markdown","metadata":{"id":"Uj56E7NDif4x"},"source":["# Guide"]},{"cell_type":"markdown","metadata":{"id":"3FribYGVdO3x"},"source":["In this practice, we will cover the following topics:\n","\n","* Linear Regression and\n","* Principal Components Analysis.\n","\n","First we are\n","going to use least squares to \"solve\" an overdetermined system of equations,\n","arising in a parameter estimation problem. The second part of the practice is\n","about Principal Component Analysis, a linear method of dimensionality reduction.\n","We will see how this can be applied to face recognition.\n","\n","\\\\\n","\n","For any doubts before and after the practice, you can contact your teacher:\n","\n","Nneka Okolo - nnekamaureen.okolo@upf.edu\n","\n","Pablo Arias - pablo.arias@upf.edu\n","\n","Adriano Pastore - adriano.pastore@upf.edu\n","\n","\\\\\n","\n","**Deadlines**: See\n","[P101](https://calendar.google.com/calendar/embed?src=c_b679939a9db8a1d8cd9f01f62d373d173f76794e4137c40e793a8d2cb11708f8%40group.calendar.google.com&ctz=Europe%2FMadrid/),\n","[P102](https://calendar.google.com/calendar/embed?src=c_5a65338fe8c3ce7909e62bb6b572b1a61ff4ad3543b12f72468e1a16bca41bd0%40group.calendar.google.com&ctz=Europe%2FMadrid),\n","[P201](https://calendar.google.com/calendar/embed?src=c_58aa336a0c5d0a38b13dd4a38071e7d8f9a18f4306ffeef2e48276087c339163%40group.calendar.google.com&ctz=Europe%2FMadrid),\n","[P202](https://calendar.google.com/calendar/embed?src=c_dac1d492e1060f3cee35420a9c2ff0d345e89a002cc8c70fe74bf0b78bf99d37%40group.calendar.google.com&ctz=Europe%2FMadrid),\n","\n","\n","\\\\\n","\n","**Submission instructions**\n","\n","Register your group members [here](https://forms.gle/NLeYqhN6LyPnSPg78) if you haven't already.\n","\n","Complete the code and answer the questions below.\n","\n","Export the notebook with the answers using the menu option File->Download .ipynb.\n","\n","Rename exported notebook with the format **lastnameUid.ipynb** where lastname is the first surname of **Member 1** in the form and Uid is their UPF ID.\n","\n","Submit your solution [here](https://forms.gle/AdYQwDEjAta1QaRY6) by the deadline. **Only one member needs to complete this step**.\n","\n","You will receive an acknowledgement of receipt.\n","\n","\\\\\n","\n","**Grading**:\n","\n","  The evaluation is based on results, conclusions and the commented code together.\n","\n","\n","\n","[comment]: <> (Macros:)\n","$\\newcommand{\\ma}[1]{\\boldsymbol{#1}}\n","\\newcommand{\\tras}[1]{#1^{\\mathrm{T}}}\n","\\newcommand{\\herm}[1]{#1^{\\mathrm{H}}}\n","\\newcommand{\\con}[1]{#1^{\\mathrm{*}}}\n","\\newcommand{\\E}{\\mathbb{E}}\n","\\newcommand{\\tech}[1]{\\overline{#1}}\n","\\newcommand{\\nspace}{\\!\\!\\!\\!}\n","\\newcommand{\\nmbr}[1]{\\oldstylenums{#1}}\n","\\newcommand{\\eg}{\\emph{e.g}. } \\newcommand{\\Eg}{\\emph{E.g}. }\n","\\newcommand{\\ie}{\\emph{i.e}. } \\newcommand{\\Ie}{\\emph{I.e}. }\n","\\newcommand{\\cf}{\\emph{c.f}. } \\newcommand{\\Cf}{\\emph{C.f}. }\n","\\newcommand{\\etc}{\\emph{etc}. } \\newcommand{\\vs}{\\emph{vs}. }\n","\\newcommand{\\wrt}{w.r.t\\onedot } \\newcommand{\\dof}{d.o.f. }\n","\\newcommand{\\etal}{\\emph{et al}. }\n","\\newcommand{\\R}{\\mathbb{R}}\n","\\newcommand{\\sign}{\\mathrm{sign}}\n","\\newcommand{\\eps}{\\varepsilon}\n","\\newcommand{\\To}{\\longrightarrow}\n","\\DeclareMathOperator*{\\argmin}{arg\\,min}\n","\\DeclareMathOperator*{\\argmax}{arg\\,max}$"]},{"cell_type":"markdown","metadata":{"id":"GSpI8DTGFEE9"},"source":["**Instructions for answering the questions.**\n","\n","Questions are indicated in blue. Some questions require answers in the form of text, some others require completing code. See the examples below. *Please do not modify the notebook outside of these cells.*"]},{"cell_type":"markdown","metadata":{"id":"0nvLHAH6FLf7"},"source":["<font color='blue'>**(QUESTION 42)** Based on what you know at this moment, answer these questions:\n","1. What are your favorite subjects?\n","2. What are your favourite hobbies?\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"Sde7xnQqFU-Q"},"source":["<font color='red'>**ANSWER**</font>\n","\n","1. I only like one subject: \"Optimization Techniques.\"\n","1. I like writing equations $e^{i\\pi} + 1 = 0$"]},{"cell_type":"markdown","metadata":{"id":"A8_KyrViFRjL"},"source":["<font color='blue'>**(QUESTION 43)** This is a coding question. There is no <font color='red'>**ANSWER**</font> cell. Instead, you should complete the code cell following the question. Typically, you'll find TODOs in the code indicating the places that you are expected to complete.\n","</font>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"H4zuDdalFX8_"},"outputs":[],"source":["a = None     # TODO substitute the None by a nice number to print\n","print(\"The number a is {}\".format(a))"]},{"cell_type":"markdown","metadata":{"id":"uupI8V2DhhKr"},"source":["# Part 1: Linear Regression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5LOKgj-sCGCi"},"outputs":[],"source":["# import required libraries\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import matplotlib.image as mpimg\n","# %matplotlib inline"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oPIoHEqqGyPJ"},"outputs":[],"source":["# !sudo apt install cm-super dvipng texlive-latex-extra texlive-latex-recommended"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LBPpYrw_IDFo"},"outputs":[],"source":["# Use latex in plots\n","plt.rcParams['text.usetex'] = True"]},{"cell_type":"markdown","metadata":{"id":"sbxOlKGGd5yY"},"source":["Suppose we have $m$ samples $x_i,y_i\\in\\mathbb R$, $i = 1,\\dots,m$, as shown in\n","Figure 1 below. Assume that we know that there is a functional\n","dependence between $x$ and $y$: $y = f(x)$. In general we do not know $f$, but\n","we know that it belongs to a certain class of functions (for example, we know\n","that the dependence between $x$ and $y$ should be logarithmic, or polynomial).\n","In order to approximate $f$, we\n","will use a *regression* technique."]},{"cell_type":"markdown","metadata":{"id":"8N19ufzph4Pb"},"source":["In this Assignment, we will use linear regression to fit a polynomial of degree 3 to the data, given by:\n","\n","$$\\hat f(x) = w_0 + w_1 x + w_2x^2 + w_3 x^3.$$\n","\n","Observe that $\\hat f$ is not a linear function of $x$, but it is linear on the\n","coeffients $w_i$. The coefficients are unknowns we have to determine. We will do so by minimizing the sum of squared errors, between the predicted value $\\hat f(x)$ and the measured value, $y$:\n","\n","$$J(w_0,w_1,w_2,w_3) = \\sum_{i = 1}^m |y_i - \\hat f(x_i)|^2 = \\sum_{i = 1}^m\n","\\left|y_i - (w_0 + w_1 x_i + w_2x_i^2 + w_3 x_i^3)\\right|^2$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ULLB-ETIC__o"},"outputs":[],"source":["# Example data\n","m = 10\n","np.random.seed(4)\n","\n","# Define x --> (m, 1)\n","x = np.random.randint(-2,3+1,(m,1))\n","y = np.power(x,3) - 0.9*np.power(x,2) + np.random.uniform(-5,5,(m,1))\n","\n","fig, ax = plt.subplots(1, 1, figsize = (7, 5))\n","ax.set_xlabel(\"x\")\n","ax.set_ylabel(\"y\")\n","ax.set_title(\"Plot of data\")\n","\n","ax.plot(np.arange(-3,4,0.05), np.power(np.arange(-3,4,0.05),3),\n","        label = \"$y = x^3$\", color = \"cornflowerblue\", alpha = 0.5)\n","ax.scatter(x, y, label = \"data\", color = \"red\")\n","\n","plt.legend()\n","\n","caption = 'Figure 1: The problem of regression. We want to fit a function $\\hat f$ to\\\n"," some data $x_i,y_i$, $i = 1,\\dots,m$.\\nThe function should be chosen to\\\n"," minimize the error between $\\hat{f}(x_i)$ and $y_i$.'\n","plt.figtext(0.5, -0.08, caption, wrap=True, horizontalalignment='center', fontsize=12)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"cao4OQkvnrRZ"},"source":["We can express $J$ in matrix notation as\n","\n","$$J(\\ma w) = \\|\\ma y - \\ma \\Phi \\ma w\\|^2$$\n","\n","where $\\ma y = [y_1,y_2,\\dots,y_m]^T\\in\\mathbb R^m$, $\\ma w = [w_0,w_1,w_2,w_3]^T\\in\\mathbb R^n$ ($n = 4$) and\n","\n","$$\\ma \\Phi = \\left[\n","\\begin{array}{c c c c }\n","\t1 & x_1 &  x_1^2 & x_1^3 \\\\\n","\t1 & x_2 &  x_2^2 & x_2^3 \\\\\n","\t\\vdots & \\vdots  & \\vdots & \\vdots \\\\\n","\t1 &  x_m & x_m^2 & x_m^3 \\\\\n","\\end{array}\n","\\right]$$\n","\n","is an $m\\times n$ matrix, called the \\emph{design matrix}. In general we will\n","have more measurements than parameters: $m >> n$.\n","\n","We are going to compute the minimum of $J$ using two methods: the normal\n","equations and the SVD."]},{"cell_type":"markdown","metadata":{"id":"Ap-B4Tqrq7Sv"},"source":["## Computing the minimum via the normal equations"]},{"cell_type":"markdown","metadata":{"id":"qzCl_aP-rwsf"},"source":["\n","\n","We seek for the coefficient vector $\\ma w^*$ which minimizes $J$:\n","\n","$$\\ma w^* = \\argmin_{\\ma w} J(\\ma w) = \\argmin_{\\ma w} \\|\\ma \\Phi \\ma w - \\ma y\\|^2$$\n","\n","As you saw in class, <font color='red'>if $\\ma \\Phi^T\\ma \\Phi$ is invertible</font>, the optimal coefficient vector $\\ma w^*$ can be computed from the following equation:\n","\n","\\begin{equation}\n","\\ma w^* = (\\ma \\Phi^T\\ma \\Phi)^{-1}\\ma \\Phi^T \\ma y.\n","\\end{equation}\n","\n","These are the *normal equations*.  Recall that $\\Phi\\ma w^*$ is the projection of $\\ma y$ over\n","$\\text{Im}\\ma \\Phi$."]},{"cell_type":"markdown","metadata":{"id":"vQQKuLvk9wwx"},"source":["<font color='blue'>**TOY EXAMPLE.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_y4S7vGhsIVk"},"outputs":[],"source":["# Define the number of coefficients to calculate (components of w --> w = (w0, w1, ..., wn))\n","num_coefficients = 4\n","\n","# Calculate Phi using x defined above\n","Phi = np.power(x, np.arange(num_coefficients))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R4jeEouRspdp"},"outputs":[],"source":["print(\"Independent variable: \\n\",x,\"\\n\\nDesign matrix:\\n\",Phi)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0hdD69ensvFS"},"outputs":[],"source":["# Calculate w* using y defined above. First check that $Phi.TPhi$ is invertible.\n","if np.linalg.det(Phi.T@Phi) == 0:\n","  print('It is not invertible')\n","w_ast = np.linalg.inv(Phi.T@Phi) @ Phi.T @ y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fnwGlJsAJ0vg"},"outputs":[],"source":["print(f\"w*:\\n{w_ast}\")"]},{"cell_type":"markdown","metadata":{"id":"uCLkpCEXtXii"},"source":["<font color='blue'>**(QUESTION 1.1)** Complete the Python function \"polyfit_inv_normal_eq\" for computing $\\ma w^*$. Follow the comments provided in the code."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vOjfBTS5CZMo"},"outputs":[],"source":["def polyfit_inv_normal_eq(x: np.ndarray,\n","                          y: np.ndarray,\n","                          n: int):\n","    \"\"\"\n","    Fits a polynomial of degree n to a sets of samples x and y.\n","    The polynomial minimizes the sum of squared errors (least squares)\n","    solving the normal equations.\n","\n","    :param x: m x 1, points in the x axis where the function is known\n","    :param y: m x 1, known values of the function at positions in x\n","    :param n: number of coefficients (degree of the polynomial + 1)\n","\n","    :return w: n x 1, vector of polynomial coefficients\n","    :return Phi: m x n, data array\n","    \"\"\"\n","    # TODO: create phi\n","    Phi = np.power(x, np.arange(n))\n","\n","    # TODO: create w*\n","    # As shown in lectures, iff A.T@A is invertible then A+ = (A.T@A)^-1 @ A.T but we are asked\n","    # to do is specifically with the normal equations\n","    w_ast = np.linalg.inv(Phi.T@Phi) @ Phi.T @ y if np.linalg.det(Phi.T@Phi) != 0 else None\n","\n","    return Phi, w_ast"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ShBjwiGttGK7"},"outputs":[],"source":["Phi, w_ast1 = polyfit_inv_normal_eq(x, y, num_coefficients)\n","print(f\"$\\Phi$:\\n{Phi}\\n\\n$w*$:\\n{w_ast1}\")"]},{"cell_type":"markdown","metadata":{"id":"6jMbURPHteC1"},"source":["<font color='blue'>**(QUESTION 1.2)**  Complete the Python function \"polyfit_main\", to\n","verify that the $\\ma w^*$ returned by \"polyfit_inv_normal_eq\" satisfies\n","that the residue $r = \\ma y - \\ma \\Phi \\ma w^*$ is orthogonal to\n","$\\text{Im}\\ma \\Phi$, \\ie $r \\perp \\text{Im}\\ma \\Phi$.  Hint:\n","$\\text{Im}\\ma \\Phi$ is the space generated by the columns of $\\ma \\Phi$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hlqgblittfii"},"outputs":[],"source":["def polyfit_main(n_data_samples: int = 100,\n","                 polynomial_size: int = 4,\n","                 lower_data_value: int = -10,\n","                 higher_data_value: int = 10):\n","    \"\"\"\n","    Fits a polynomial of degree n to a set of samples.\n","\n","    :param n_data_samples: number of samples to be created\n","    :param polynomial_size: polynomial size (number of elements in array)\n","    :param lower_data_value: lowest value in data\n","    :param higher_data_value: highest value in data\n","\n","    :return:\n","    \"\"\"\n","    # TODO: create x\n","    x = np.random.randint(lower_data_value, higher_data_value, (n_data_samples, 1))\n","\n","    # TODO: create y to fit p(x) = w0 + w1 * x + w2 * x^2 + w3 * x ^ 3 + ... + wn * x ^ n --> n = polynomial_size\n","    Phi = np.power(x, np.arange(polynomial_size))\n","    y = Phi @ np.random.uniform(-5, 5, (polynomial_size, 1))\n","\n","    \"\"\"Plot the data\"\"\"\n","    plt.figure()\n","    plt.plot(x, y, '.')\n","    plt.show()\n","\n","    # TODO: Calculate w*\n","    _, w_ast = polyfit_inv_normal_eq(x, y, polynomial_size)\n","\n","    # TODO: Calculate the residue and demostrate it is orthogonal to Phi\n","    y_hat = Phi @ w_ast\n","    residue = y - y_hat # strange that not squared, but following the handout should be this\n","    scalar_product = Phi.T @ residue # transpose is equivalent to multiply each column of Phi to the residue\n","    scalar_value = np.sqrt(scalar_product.T@scalar_product).flatten()\n","    print(\"Scalar product array: \\n\\n{}\\n\".format(scalar_product))\n","    print(\"Scalar product solution: {}\".format(scalar_value))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BLDolDsitnfK"},"outputs":[],"source":["polyfit_main()"]},{"cell_type":"markdown","metadata":{},"source":["<font color='red'>**ANSWER**</font>\n","\n","We can see that the scalar product is approximately 0 in all the dimensions and has magnitude approximately 0 too, therefore the residual and the image of the design matrix Phi are orthogonal to each other, satisfying that the distance is minimal."]},{"cell_type":"markdown","metadata":{"id":"5k8VPM8ztqzy"},"source":["## Minimization with the SVD"]},{"cell_type":"markdown","metadata":{"id":"JBUpcVZotwKG"},"source":["Note that, for solving the normal equations using the closed form expression of $\\ma w^*$ the matrix $\\ma \\Phi^T\\ma \\Phi$ needs to be\n","invertible. Let us now derive a different way to compute $\\ma w^*$ based on\n","the Singular Value Decomposition (SVD), which can be extended even to the case\n","in which $\\ma \\Phi^T\\ma \\Phi$ is not invertible.  \n","\n","Using the SVD decomposition, we can express\n","$$\\ma \\Phi = U S V^T$$\n","where $U\\in \\mathbb R^{m\\times m}$ and $V\\in \\mathbb R^{n\\times n}$ are orthogonal\n","matrices (a square matrix $A\\in \\mathbb R^{m\\times m}$ is orthogonal when $A^T A =\n","A A^T = I_m$), and $S\\in\\mathbb R^{m\\times n}$ is a rectangular diagonal\n","matrix. The diagonal values of $S$, $s_{ii} = \\sigma_i > 0$, for $i = 1,\\dots,q$\n","are the singular values. The number of singular values $q$ is the rank of $\\ma\n","\\Phi$, thus, $q \\leqslant \\min \\{m,n\\}$.\n","\n","We will use the SVD to compute the \\emph{pseudo-inverse} of $\\ma \\Phi$, $\\ma\n","\\Phi^\\dagger$. Let us first recall what is the pseudo-inverse of a matrix, and\n","how it is computed."]},{"cell_type":"markdown","metadata":{"id":"9mtZIrTMv8tM"},"source":["### The Pseudo-inverse"]},{"cell_type":"markdown","metadata":{"id":"lZcBbbZSwQYq"},"source":["We will define define pseudo-inverse in two steps: first we are going to define\n","the pseudo-inverse of a diagonal matrix. Then we are going to extend this\n","definition to any matrix.\n","\n","\n","*Pseudo-inverse of a diagonal matrix.* Let $S$ be an $m\\times\n","n$ rectangular diagonal matrix. The pseudo-inverse of $S$, $S^\\dagger$,\n","is also a diagonal matrix, but with dimensions $n\\times m$. The diagonal elements\n","of $S^\\dagger$ are computed as follows\n","\n","$$s^\\dagger_{ii} = \\left\\{\n","\\begin{array}{l l}\n","\ts_{ii}^{-1} & \\text{if }s_{ii} \\neq 0\\\\\n","\t0 & \\text{if }s_{ii} = 0,\n","\\end{array}\n","\\right.$$\n","\n","for $i = 1,\\dots, \\min(m,n)$."]},{"cell_type":"markdown","metadata":{"id":"rt_UPAN-wlKP"},"source":["*Pseudo-inverse of any matrix.* Let $A$ be an $m\\times n$\n","matrix. We define its pseudo-inverse based on the SVD decomposition and\n","the previous definition of the pseudo-inverse of a diagonal matrix. Using the\n","SVD decomposition we can express $A = USV^T$. We then define\n","\n","$$A^\\dagger = VS^\\dagger U^T.$$\n","\n","Note that $S$ is a diagonal matrix with the singular values on the diagonal, so\n","we already know how to compute its pseudo-inverse."]},{"cell_type":"markdown","metadata":{"id":"xrr_cmYQwyuv"},"source":["*Note.* The pseudo-inverse is a generalization of the inverse\n","which applies to any matrix (recall that only some square matrices are\n","invertible). If a matrix $A$ is invertible, we have that $S^\\dagger = S^{-1}$.\n","The pseudo-inverse has many interesting properties. In the following we will\n","make use of one of them."]},{"cell_type":"markdown","metadata":{"id":"Is3UpW28w4jp"},"source":["<font color='blue'>**(QUESTION 1.3)**  Generate a $5\\times 3$ random matrix, $A$. Compute its\n","SVD $A = USV^T$ using the command \"svd\". Verify that the $U$ and $V$ are\n","orthogonal matrices and $S$ is diagonal. Verify that $A = USV^T$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oWP7OtMquFpg"},"outputs":[],"source":["# Generate 5×3  random matrix of type np.ndarray\n","A = None\n","print(A)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a2qpTOD8ugIS"},"outputs":[],"source":["# Compute its SVD\n","U, S, Vt = None### WRITE YOUR SOLUTION\n","print(\"U: \\n\\n{}\\n\\nS: \\n\\n{}\\n\\nVt: \\n\\n{}\".format(U, S, Vt))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HDweOgKXunMA"},"outputs":[],"source":["# Verify that U and V are orthogonal\n","UTU = None ### WRITE YOUR SOLUTION\n","UUT = None ### WRITE YOUR SOLUTION\n","VTV = None ### WRITE YOUR SOLUTION\n","VVT = None ### WRITE YOUR SOLUTION\n","\n","print(\"UTU\\n\\n{}\\n\\nUUT\\n\\n{}\\n\\nVTV\\n\\n{}\\n\\nVVT\\n\\n{}\\n\\nAre U and V orthogonal matrices? {}\".format(UTU, UUT, VTV, VVT, np.allclose(UTU, UUT) and np.allclose(VTV, VVT)))"]},{"cell_type":"markdown","metadata":{"id":"Pv_qySJfuYdB"},"source":["<font color='blue'>**(QUESTION 1.4a)** Compute the pseudo-inverse of $S$, $S^\\dagger$. Use it\n","to compute the pseudo-inverse of $A$, $A^\\dagger$. Verify that $A^\\dagger A$\n","is the identity matrix of size $3$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j8xWpjBeuQXk"},"outputs":[],"source":["# Compute pseudo-inverse of  𝑆\n","S_pinv = None ### WRITE YOUR SOLUTION\n","print(S_pinv)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qAuVcEYUyppI"},"outputs":[],"source":["# Compute pseudo-inverse of  𝐴\n","A_pinv = None\n","print(A_pinv)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6DrWznnsyx4U"},"outputs":[],"source":["# Verify that A†A is the identity matrix of size 3 of type np.ndarray\n","I_ = None ### WRITE YOUR SOLUTION\n","print(\"A†A:\\n\\n{}\\n\".format(I_))\n","print(\"A†A is the identity matrix of size 3?: {}\".format(np.allclose(I_, np.identity(n = len(I_)))))"]},{"cell_type":"markdown","metadata":{"id":"JbBBDegLMA5Y"},"source":["<font color='blue'>**(QUESTION 1.4b)** What happens with $AA^\\dagger $?"]},{"cell_type":"markdown","metadata":{"id":"hGUU-2HoMC2U"},"source":["<font color='red'>**ANSWER:**"]},{"cell_type":"markdown","metadata":{"id":"FaDrut0IxKPW"},"source":["### Solving least squares with the pseudo-inverse"]},{"cell_type":"markdown","metadata":{"id":"1OFx5HRhxTTK"},"source":["It turns out that we can compute the solution $\\ma w^*$ to the least squares\n","problem using the pseudo-inverse of the design matrix $\\ma \\Phi$ as follows:\n","\n","$$\\ma w^* = \\ma \\Phi^{\\dagger}\\ma y = VS^\\dagger U^T\\ma y.$$"]},{"cell_type":"markdown","metadata":{"id":"6DKfLc3Ky_Sf"},"source":["<font color='blue'>**(QUESTION 1.5)** Complete the Python function \"polyfit_svd_normal_eq\" for computing $\\ma w^*$. Follow the comments provided in the code.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WI2EpFmsy7Nz"},"outputs":[],"source":["def polyfit_svd_normal_eq(x: np.ndarray,\n","                          y: np.ndarray,\n","                          n: int):\n","    \"\"\"\n","    Fits a polynomial of degree n to a sets of samples x and y. The polynomial minimizes the sum\n","    of squared errors (least squares) using the pseudoinverse of data array.\n","\n","    :param x: m x 1, points in the x axis where the function is known\n","    :param y: m x 1, known values of the function at positions in x\n","    :param n: number of coefficients (degree of the polynomial + 1)\n","\n","    :return w_ast: n x 1, vector of polynomial coefficients\n","    :return Phi_pinv: n x m, pseudo-inverse of Phi\n","    :return Phi: m x n, data array\n","    \"\"\"\n","    # TODO: Calculate Phi\n","    Phi = None ### WRITE YOUR SOLUTION\n","    # TODO: Calculate its SVD\n","    U, S, Vt = None, None, None ### WRITE YOUR SOLUTION\n","\n","    # TODO: Calculate the pseudo-inverse of S\n","    S_pinv = None ### WRITE YOUR SOLUTION\n","\n","    # TODO: Calculate w*\n","    Phi_pinv = None ### WRITE YOUR SOLUTION\n","    w_ast = None ### WRITE YOUR SOLUTION\n","\n","    return Phi, Phi_pinv, w_ast"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fDBOSPAOz1dq"},"outputs":[],"source":["Phi, Phi_pinv, w_ast2 = polyfit_svd_normal_eq(x, y, num_coefficients)\n","print(f\"$\\Phi$:\\n{Phi}\\n\\n$\\Phi^\\dagger$:\\n{Phi_pinv}\\n\\n$w*$:\\n{w_ast2}\")"]},{"cell_type":"markdown","metadata":{"id":"n5eEJpNpzDOb"},"source":["<font color='blue'>**(QUESTION 1.6)** Compare with the solution obtained by inverting the\n","normal equations."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eElkSUiLzXVi"},"outputs":[],"source":["print(f\"Solution is similar with inverting normal equations: {np.allclose(w_ast1, w_ast2)}\")"]},{"cell_type":"markdown","metadata":{"id":"K0aii4lzx2QR"},"source":["# Part 2: Principal Components Analysis"]},{"cell_type":"markdown","metadata":{"id":"Z4kPL2cyyCGt"},"source":["In this section we are going to explore the application of Principal Components\n","Analysis (PCA) for *dimensionality reduction*. Imagine we have a data set\n","formed by several (say $m$) points in $\\mathbb R^n$, $\\mathcal X =\n","\\{x_1,\\dots,x_m\\}$. For example, in Figure 2 below we show a set of\n","points in $\\mathbb R^2$. Another example, in the following section, the\n","\"points\" will be $m$ images of faces. Since the points are in $\\mathbb R^n$\n","we are using $n$ coefficients to represent them. In many cases, we would be\n","interested in finding a representation of the dataset, which allows to encode\n","each point $x_i$ using $p << n$ coefficients.\n","\n","Let us assume for simplicity that the points are centered at the origin (\\ie\n","the mean or barycenter is at the origin: $\\sum_i x_i = 0$). The idea behind PCA\n","is to approximate the point set by projecting it on a $p$ dimensional subspace\n","$V_p$, with $p \\leqslant n$."]},{"cell_type":"markdown","metadata":{"id":"xJZHCmTZTkrb"},"source":["<figure>\n","  <img src=\"https://github.com/Muchay/OptTechCourse_Aux/blob/main/Lab1/images/PCA_img.png?raw=true\" width=\"50%\">\n","  <figcaption>Figure 2: PCA of a two dimensional point set. The principal directions $v_1,v_2$ form an orthogonal basis of $R^2$. Note that $v_1$ \"aligns\" with the point set: most of the variation of the points is along the direction $v_1$. These directions are chosen to minimize the mean squared projection error. In the Figure we show the projection errors over the principal direction $v_1$ for two points (red lines).</figcaption>\n","</figure>"]},{"cell_type":"markdown","metadata":{"id":"f3J6yQZUy2_r"},"source":["To do that, we will build an orthonormal basis $\\mathcal V_n =\n","\\{v_1,v_2,\\dots,v_n\\}$ of $\\mathbb R^n$, specially designed to adapt to the dataset (these\n","are the red vectors $v_1,v_2$ in Figure 2).\n","\n","Any point $x_i$ from the data set can be expressed by its $n$ coordinates\n","on the basis:\n","$[\\langle x_i,v_1\\rangle,\\dots,\\langle x_i,v_n\\rangle]^T\\in\\mathbb R^n$.  This means that we\n","can recover $x_i$ as\n","$$x_i = \\sum_{j = 1}^n \\langle x_i,v_j\\rangle v_j.$$\n","If we only keep the first $p$ coefficients $[\\langle x_i,v_1\\rangle,\\dots,\\langle\n","x_i,v_p\\rangle]^T$, we recover the projection of $x_i$ over $V_p$, the subspace\n","spanned by the first $p$ vectors in the basis:\n","\n","$$P_{V_p} (x_i) = \\sum_{j = 1}^p\n","\\langle x_i,v_j\\rangle v_j.$$\n","\n","What we want to find is a basis that best approximates the dataset. Meaning that\n","for each $p \\leqslant n$, $V_p$ is the $p$-dimensional vector space minimizing\n","the mean squared projection error\n","\n","$$\\frac1m\\sum_{i = 1}^m \\|P_{V_p}(x_i) - x_i\\|^2.$$"]},{"cell_type":"markdown","metadata":{"id":"vWntBlk-zirW"},"source":["The resulting vectors $v_1,\\dots,v_n$ are called the \\emph{principal\n","directions} of the set of points $\\mathcal X= \\{x_1,\\dots,x_m\\}$. For any\n","$x\\in \\mathbb R^n$, the coefficients $\\langle x,v_1\\rangle,\\dots,\\langle\n","x,v_n\\rangle$ are called the \\emph{principal components} of $x$."]},{"cell_type":"markdown","metadata":{"id":"i-Vw45Oo_Eie"},"source":["## Computation and Properties of the Principal Components"]},{"cell_type":"markdown","metadata":{"id":"C1abh7pc_aqE"},"source":["<font color='blue'>**TOY EXAMPLE.**\n","\n","Let us consider a data set $\\mathcal X = \\{x_1,\\dots,x_m\\}\\subset \\mathbb R^n$.\n","Let $\\ma X$ be the data matrix: the rows of $\\ma X$ are the vectors $x_i\\in\n","\\mathcal X$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6gS7Ge7q3tH9"},"outputs":[],"source":["# Define x --> (m, n)\n","m, n = 5, 3\n","x = None ### WRITE YOUR SOLUTION\n","print(\"X: \\n\\n{}\\n\".format(x))\n","\n","# Calculate its mean\n","mu = None ### WRITE YOUR SOLUTION\n","print(\"Mean: {}\\n\".format(mu))\n","\n","# Center X\n","x_centered = x - mu\n","print(\"X centered: \\n\\n{}\\n\".format(x_centered))"]},{"cell_type":"markdown","metadata":{"id":"V6E0qAtK3rYz"},"source":["1.   The principal directions of $\\mathcal X$ form an orthonormal basis\n","$v_1,\\dots,v_n$ of $\\mathbb R^n$ given by the eigenvectors of the\n","empirical covariance matrix $\\frac1{m-1}\\ma X^T\\ma X$. Let $\\lambda_1\\geqslant\n","\\lambda_2\\geqslant \\dots \\geqslant \\lambda_n \\geqslant 0$ be the\n","corresponding eigenvalues."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KBwYM_UN4B5p"},"outputs":[],"source":["# Calculate the empirical covariance matrix of type np.ndarray\n","C = None ### WRITE YOUR SOLUTION\n","print(\"Covariance matrix: \\n\\n{}\".format(C))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OK0Rpp504PmC"},"outputs":[],"source":["# Define the number of p principal directions to be retrieved\n","p = 2 #TRY WITH p=n\n","\n","# Compute the principal directions, sort them by lambda values (eigen values) and return only\n","# the p-first values\n","# 1. Get the eigen values and vectors\n","eigen_values, eigen_vectors = None, None ### WRITE YOUR SOLUTION\n","\n","# 2. Sort them by eigen values and return only the p-first values\n","idx = None ### WRITE YOUR SOLUTION\n","eigen_values = None ### WRITE YOUR SOLUTION\n","eigen_vectors = None ### WRITE YOUR SOLUTION"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4FSvDyJG4iUr"},"outputs":[],"source":["print(\"Eigen values: \\n\\n{}\\n\\nEigen vectors: \\n\\n{}\".format(eigen_values, eigen_vectors))"]},{"cell_type":"markdown","metadata":{"id":"I1798vym39KM"},"source":["2.   For $p = 1,\\dots,n$, the subspace $V_p$ spanned by the first $p$\n","principal directions is the $p$-dimensional subspace which minimizes the\n","mean squared projection error:\n","\n","$$\\frac1m\\sum_{i = 1}^m \\|x_i - P_{V_p}(x_i)\\|^2 = \\sum_{j = p+1}^n \\lambda_j.$$"]},{"cell_type":"markdown","metadata":{"id":"aWmptdIX4AyN"},"source":["3.   Equivalently $V_p$ is also the $p$-dimensional subspace that maximizes the\n","projection variance:\n","\n","$$\\frac1m\\sum_{i = 1}^m \\| P_{V_p}(x_i)\\|^2 = \\sum_{j = 1}^p \\lambda_j$$"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XOLvz_Pud76b"},"outputs":[],"source":["# Calculate the projection variance\n","proj_var = None ### WRITE YOUR SOLUTION\n","print(\"Projection variance: \\n\\n{}\".format(proj_var))"]},{"cell_type":"markdown","metadata":{"id":"a5gPo_b14r_C"},"source":["<font color='blue'>**(QUESTION 2.1)** Complete the Python function \"pca_prin_dir\" for\n","computing the $p$ first principal directions via the eigenvectors of $\\frac1{m-1}\\ma X^T\\ma X$.  Follow the comments in the code.\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PqURACGJ43Ia"},"outputs":[],"source":["def pca_prin_dir(x: np.ndarray,\n","                 p: int):\n","    \"\"\"\n","    Computes the principal directions, variances and mean of vectors in x.\n","    x is a row data array: its rows are vectors xi. The principal directions are given\n","    by eigenvectors of the empirical covariance matrix, x'*x.\n","\n","    :param x: row data array, m x n\n","    :param p: number of principal directions\n","\n","    :return eigen_vectors: principal direction array (each column is a PD) n x p\n","    :return eigen_values: eigenvalue diagonal array p x p\n","    :return mu: mean\n","    \"\"\"\n","    # TODO: Calculate the mean of the incoming data and center x\n","    mu = None ### WRITE YOUR SOLUTION\n","    x_centered = None ### WRITE YOUR SOLUTION\n","\n","    # TODO: Calculate the empirical covariance matrix\n","    C = None ### WRITE YOUR SOLUTION\n","\n","    # TODO: Compute the principal directions and sort them by lambda values (eigen values) and return only\n","    # the p-first values\n","    # 1. Get the eigen values and vectors\n","    eigen_values, eigen_vectors = None, None ### WRITE YOUR SOLUTION\n","\n","    # 2. Sort them by eigen values and return only the p-first values\n","    idx = None ### WRITE YOUR SOLUTION\n","    eigen_values = None ### WRITE YOUR SOLUTION\n","    eigen_vectors = None ### WRITE YOUR SOLUTION\n","\n","    return eigen_vectors, eigen_values, mu"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pCU3SxqU46pv"},"outputs":[],"source":["eigen_vectors, eigen_values, mu = pca_prin_dir(x, p)\n","print(\"Eigen values: \\n\\n{}\\n\\nEigen vectors: \\n\\n{}\\n\\nMean: \\n\\n{}\".format(eigen_values, eigen_vectors, mu))"]},{"cell_type":"markdown","metadata":{"id":"FqhJlf7E4wlt"},"source":["<font color='blue'>**(QUESTION 2.2)** Complete the Python functions \"pca_prin_comp\" for\n","computing the $p$ first principal components $[\\langle x,v_i \\rangle]_{i =\n","1,\\dots,p}$ of a point $x$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xer8DDu_5HlT"},"outputs":[],"source":["def pca_prin_comp(x: np.ndarray,\n","                  eigen_vectors: np.ndarray,\n","                  mu: np.ndarray = None):\n","    \"\"\"\n","    Computes the principal components of vectors in x. x is a row data array: its rows are vectors xi.\n","    The principal components zi of a vector xi are given by the projection over the principal directions:\n","\n","                            zi = [ <xi-mu,v1> <xi-mu,v2> ... <xi-mu,vp> ]\n","\n","    :param x: row data array, m x n\n","    :param eigen_vectors: principal direction array (each column is a PD) n x p\n","\n","    :return z : m x p, principal component array. Row i contains the princpal components of xi\n","    \"\"\"\n","    # TODO: Calculate the raw data mean along row axis and center x\n","    if mu is None:\n","      mu = None ### WRITE YOUR SOLUTION\n","\n","    x_centered = None ### WRITE YOUR SOLUTION\n","\n","    # Project x over basis elements to compute principal components\n","    z = None ### WRITE YOUR SOLUTION\n","\n","    return mu, z"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CK3UWdj_5Kii"},"outputs":[],"source":["mu, z_p = pca_prin_comp(x = x,\n","              eigen_vectors = eigen_vectors)\n","print(\"Mean: \\n\\n{}\\n\\nPrincipal components: \\n\\n{}\".format(mu, z_p))"]},{"cell_type":"markdown","metadata":{"id":"zakL3I6d4yTa"},"source":["<font color='blue'>**(QUESTION 2.3)** Complete the Python functions \"pca_reconstruct\" for\n","reconstructing a point $x$ from its principal components."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v8DxUT5s5rPX"},"outputs":[],"source":["def pca_reconstruct(z: np.ndarray([]),\n","                    eigen_vectors: np.ndarray([]),\n","                    mean: float):\n","    \"\"\"\n","    Given a set of vectors z(i,:) (rows of array z) expressed in principal components, this function computes the x(i,:), 'inverting' the PCA change of coordinates.\n","    It goes from the low dimensional PCA representation to the high dimensional vectors.\n","\n","    :param z: m x p, principal component array (row-wise).\n","    :param V: principal direction array (each column is a PD) n x p\n","    :param mu: mean\n","\n","    :return x: row data array, m x n. Row x(i,:) is the high dimensional reconstruction from z(i,:)\n","    \"\"\"\n","    # Calculate x_hat\n","    x_hat = None ### WRITE YOUR SOLUTION\n","\n","    # Add the mean to recover the real array values\n","    x_ = None ### WRITE YOUR SOLUTION\n","\n","    return x_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7fcFSREg5s4Z"},"outputs":[],"source":["pca_reconstruct(z = z_p, eigen_vectors = eigen_vectors, mean = mu)"]},{"cell_type":"markdown","metadata":{"id":"OgJTROqr40qa"},"source":["## Example with a dataset"]},{"cell_type":"markdown","metadata":{"id":"MJHUOfZtTWow"},"source":["First we need some helper files:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1qC-F-nNTZ0H"},"outputs":[],"source":["# Mount Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O9jd3ctwTabn"},"outputs":[],"source":["# Move to your drive\n","%cd /content/drive/MyDrive/"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vYherhRmTcUn"},"outputs":[],"source":["# Clone repo with auxilary files required for this and the following labs.\n","# This needs to be done only once for the course.\n","!git clone https://github.com/Muchay/OptTechCourse_Aux.git"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aZvyZlHpTfu_"},"outputs":[],"source":["# Create source path\n","source = \"/content/drive/MyDrive/OptTechCourse_Aux/Lab1/\""]},{"cell_type":"markdown","metadata":{"id":"oue8R6_KVLUz"},"source":["Next, we install and import required libraries, and import the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iLj_fom56GdI"},"outputs":[],"source":["# Install a widget that will allow you to build up interactive plots\n","# !conda install -c conda-forge ipympl -y\n","\n","# If using JupyterLab\n","# !conda install -c conda-forge nodejs -y\n","# !jupyter labextension install @jupyter-widgets/jupyterlab-manager jupyter-matplotlib\n","\n","# After run this lines, close the jupyter session and restore it"]},{"cell_type":"markdown","metadata":{"id":"sPfZ468D6xnH"},"source":["**1) Read the CSV file which contains the data and plot them**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"idRU4Q7z6JY_"},"outputs":[],"source":["#pip install mpld3\n","#pip install ipympl"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OnN6z5Dg66DY"},"outputs":[],"source":["# Import Pandas --> see https://pandas.pydata.org/\n","import pandas as pd\n","import seaborn as sns\n","from mpl_toolkits.mplot3d import Axes3D\n","import mpld3\n","import matplotlib.cm as cm\n","%matplotlib widget"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zKPuvTFr69oE"},"outputs":[],"source":["# Read the file\n","data = pd.read_csv(source+\"data/point_cloud_r3.csv\", sep = \",\", header=None, names = [\"x\", \"y\", \"z\"])\n","data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qV_tZZlSi9up"},"outputs":[],"source":["from google.colab import output\n","output.enable_custom_widget_manager()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8S29SDTN6_Vu"},"outputs":[],"source":["# Plot the data\n","fig = plt.figure(figsize=(8, 6))\n","ax = fig.add_subplot(111, projection='3d', )\n","ax.scatter(data['x'], data['y'], data['z'], s=10, alpha=0.9, edgecolors='w', depthshade = False)\n","plt.xticks(list(range(int(data['x'].min()), int(data['x'].max()), 1)))\n","plt.yticks(list(range(int(data['y'].min()), int(data['y'].max()), 1)))\n","ax.set_zticks(list(range(int(data['z'].min()), int(data['z'].max()), 1)))\n","\n","ax.set_xlabel('X')\n","ax.set_ylabel('Y')\n","ax.set_zlabel('Z')\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"LHwFskYdDQ11"},"source":["<font color='blue'>**(QUESTION 2.4)** Four figures will open. Read the code and explain what each figure is showing."]},{"cell_type":"markdown","metadata":{"id":"wA6srJTqmNlA"},"source":["<font color='red'>**ANSWER:**"]},{"cell_type":"markdown","metadata":{"id":"xLwsK6WY7ELh"},"source":["**2) Compute principal all principal directions and the correspoding eigenvalues**\n","\n","NOTE: in this example we are going to do some visualization of the principal components in IR^3. For that we will compute ALL principal directions and ALL principal components. Please note that this is not the typical way in which PCA is used. In a normal application of dimensionality reduction we would compute only a few principal directions and principal components"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0f6Iy4wz7CX9"},"outputs":[],"source":["# Define the number of principal directions\n","p = len(data.columns)\n","\n","# Calculate the PCA principal directions for the given data\n","eigen_vectors, eigen_values, mu = pca_prin_dir(data.values, p)\n","print(\"Eigen values: \\n\\n{}\\n\\nEigen vectors: \\n\\n{}\".format(eigen_values, eigen_vectors))"]},{"cell_type":"markdown","metadata":{"id":"PTZvX8fS7Or5"},"source":["<font color='blue'>**(QUESTION 2.4a)** What can you tell from the eigenvalues about the geometry of the dataset?"]},{"cell_type":"markdown","metadata":{"id":"ryfpIOOkdpeY"},"source":["<font color='red'>**ANSWER:**"]},{"cell_type":"markdown","metadata":{"id":"oU8SfHjO-tZx"},"source":["<font color='blue'>**(QUESTION 2.4b)** Which is the number of components you would keep to hold at least the 95% of the variance?\n","\n","In order to answer this question, we must represent the explained variance in terms of participation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mqSrvQW_7YZy"},"outputs":[],"source":["# Calculate the participation in terms of %\n","tot = sum(eigen_values)\n","var_exp = [(i / tot)*100 for i in sorted(eigen_values, reverse=True)]\n","cum_var_exp = np.cumsum(var_exp)\n","\n","# Plot the graphic\n","with plt.style.context('seaborn-whitegrid'):\n","    plt.figure(figsize=(6, 4))\n","\n","    plt.bar(range(len(eigen_values)), var_exp, alpha=0.5, align='center',\n","            label='individual explained variance')\n","    plt.step(range(len(eigen_values)), cum_var_exp, where='mid',\n","             label='cumulative explained variance')\n","    plt.ylabel('Explained variance ratio')\n","    plt.xlabel('Principal components')\n","    plt.legend(loc='best')\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"nC08sp8f7Z4l"},"source":["<font color='red'>**ANSWER:**"]},{"cell_type":"markdown","metadata":{"id":"XV87RTdH7gBg"},"source":["<font color='blue'>**(QUESTION 2.4c)** How large would be the mean projection error we would commit by keeping only the first principal component?\n","\n","In theory, MSE and eigenvalues are related by the following equation:\n","\n","\\begin{equation*}\n","\\frac{1}{m}\\sum^{n}_{i=1}\\|x_{i}-P_{V_{p}}(x_{i})\\|^{2}= \\sum^{n}_{j=p+1}\\lambda_{j}\n","\\end{equation*}\n","\n","Therefore, we will check that in both cases we achieve the same result."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dXIIP5GX7jC1"},"outputs":[],"source":["# Define the number of components to be kept\n","components = 2\n","components_array = list(range(components))\n","\n","# Calculate the projection of x over the eigenvectors basis by only keeping the first component\n","mu, z = pca_prin_comp(x = data.values,\n","                      eigen_vectors = eigen_vectors)\n","projection = pca_reconstruct(z = z[:, components_array], eigen_vectors = eigen_vectors[:, components_array], mean = mu)\n","\n","# Calculate the MSE\n","print(\"MSE for the first {} components: {}\".format(components, np.power(data.values - projection, 2).sum() / (len(data) - 1)))"]},{"cell_type":"markdown","metadata":{"id":"T3ieQZmA7lXM"},"source":["**Function definition**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"auSI7cOU7n-c"},"outputs":[],"source":["def calculate_projection_mse(x: np.ndarray,\n","                             eigen_vectors: np.ndarray,\n","                             z: np.ndarray,\n","                             p: np.ndarray,\n","                             mean: float):\n","    \"\"\"\n","    Calculates the projection MSE\n","\n","    :param x: row data array, m x n\n","    :param eigen_vectors: principal direction array (each column is a PD) n x p\n","    :param z: m x p, principal component array. Row i contains the princpal components of xi\n","    :param p: number of components to be used when projecting\n","    :param mu: mean\n","\n","    :return:\n","    \"\"\"\n","    # Define the number of components to be kept\n","    components_array = list(range(p))\n","\n","    # Calculate the projection of x over the eigenvectors basis by only keeping the first component\n","    mu, z = pca_prin_comp(x = x,\n","                          eigen_vectors = eigen_vectors)\n","    projection = pca_reconstruct(z = z[:, components_array], eigen_vectors = eigen_vectors[:, components_array], mean = mean)\n","\n","    # Calculate the MSE\n","    mse = np.power(x - projection, 2).sum() / (len(x) - 1)\n","\n","    return mse"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UtwIU7U97p3N"},"outputs":[],"source":["components = 1\n","print(\"MSE for the first {} components: {}\".format(components, calculate_projection_mse(x = data.values,\n","                                                                                        eigen_vectors = eigen_vectors,\n","                                                                                        z = z,\n","                                                                                        p = components,\n","                                                                                        mean = mu)))"]},{"cell_type":"markdown","metadata":{"id":"2bVn5HXbBTfe"},"source":["<font color='red'>**ANSWER:**"]},{"cell_type":"markdown","metadata":{"id":"VVFRNIwV7tjd"},"source":["<font color='blue'>**(QUESTION 2.4d)** And if we keep the first and the second?"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SPk5oVe07vvJ"},"outputs":[],"source":["components = 2\n","print(\"MSE for the first {} components: {}\".format(components, calculate_projection_mse(x = data.values,\n","                                                                                        eigen_vectors = eigen_vectors,\n","                                                                                        z = z,\n","                                                                                        p = components,\n","                                                                                        mean = mu)))"]},{"cell_type":"markdown","metadata":{"id":"k6pYmCvyBWtZ"},"source":["<font color='red'>**ANSWER:**"]},{"cell_type":"markdown","metadata":{"id":"z9ECgrwz74k0"},"source":["**3) Now plot the point cloud in 3D. Do three plots, in each of them will color the points with the value of one principal component (use the provided function plot_3d_and_components). Also plot the principal directions as vectors at the origin. The length of these vectors is proportional to the corresponding eigenvalue lambda.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eDTI2AQH8KXF"},"outputs":[],"source":["def plot_3d_and_components(xs: np.ndarray,\n","                           ys: np.ndarray,\n","                           zs: np.ndarray,\n","                           means: np.ndarray,\n","                           principal_components: np.ndarray,\n","                           p: int,\n","                           eigen_vectors: np.ndarray = None,\n","                           color_map: list = []):\n","    \"\"\"\n","    Plots a 3D graph and the PCA components\n","\n","    :param xs: x-axis data\n","    :param ys: y-axis data\n","    :param zs: z-axis data\n","    :param means: means per axis\n","    :param eigen_vectors: eigenvectors\n","    :param principal_components: principal component array\n","    :param p: number of components to use\n","    :param color_map: color map to be used when representing\n","    \"\"\"\n","    # Import libraries\n","    import matplotlib\n","    import inflect\n","\n","    # Calculate the maximum and minimum values\n","    components = list(range(p))\n","    minimas = principal_components[:, components].min(axis = 0)\n","    maximas = principal_components[:, components].max(axis = 0)\n","\n","    # Build up a object to convert from number to ordinal\n","    p = inflect.engine()\n","\n","    # Build up the colors map and plot the figures\n","    fig = plt.figure(figsize=(10, 8))\n","    xticks = list(range(int(xs.min()), int(xs.max()), 1))\n","    yticks = list(range(int(ys.min()), int(ys.max()), 1))\n","    zticks = list(range(int(zs.min()), int(zs.max()), 1))\n","    for component in components:\n","        norm = matplotlib.colors.Normalize(vmin=minimas[component], vmax=maximas[component], clip=True)\n","        mapper = cm.ScalarMappable(norm=norm, cmap=color_map[component])\n","        color = [mapper.to_rgba(v)[0] for v in z[:, [component]]]\n","\n","        # Plot the data\n","        ax = fig.add_subplot(1, len(components), component + 1, projection='3d')\n","        ax.scatter(xs, ys, zs, s=10, c = color, alpha=0.9, edgecolors='w', depthshade = False)\n","        if eigen_vectors is not None:\n","            for v in eigen_vectors.T:\n","                ax.plot([means[0] - v[0], v[0] + means[0]],\n","                        [means[1] - v[1], v[1] + means[1]],\n","                        [means[2] - v[2], v[2] + means[2]],\n","                        linewidth = 3,\n","                        color = \"darkblue\")\n","        ax.set_xticks(xticks)\n","        ax.set_yticks(yticks)\n","        ax.set_zticks(zticks)\n","        ax.set_xlabel('X')\n","        ax.set_ylabel('Y')\n","        ax.set_zlabel('Z')\n","        ax.set_title(\"Principal directions and {} principal component (color).\".format(p.ordinal(component + 1)), fontdict = {\"fontsize\": 6})\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a4N6PKBU8MC4"},"outputs":[],"source":["plot_3d_and_components(xs = data.x,\n","                       ys = data.y,\n","                       zs = data.z,\n","                       means = mu,\n","                       eigen_vectors = eigen_vectors * eigen_values * 0.5,\n","                       principal_components = z,\n","                       p = 3,\n","                       color_map = [cm.gnuplot, cm.gnuplot, cm.gnuplot])"]},{"cell_type":"markdown","metadata":{"id":"OA8wQLmr8Nnq"},"source":["**4) Now let's do some dimensionality reduction. For that, we keep some of theprincipal components z.**\n","\n","**a) Keep the first two principal components, project the points into a two dimensional vector space and calculate the projection error.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AR-eVv1k8RF1"},"outputs":[],"source":["def plot_3d_and_projections(xs: np.ndarray,\n","                            ys: np.ndarray,\n","                            zs: np.ndarray,\n","                            xp: np.ndarray,\n","                            yp: np.ndarray,\n","                            zp: np.ndarray,\n","                            title: str = \"\"):\n","    \"\"\"\n","    Plots a 3D graph and the PCA components\n","\n","    :param xs: x-axis data\n","    :param ys: y-axis data\n","    :param zs: z-axis data\n","    :param xp: x-axis projection data\n","    :param yp: y-axis projection data\n","    :param zp: z-axis projection data\n","    \"\"\"\n","    # Import libraries\n","    import matplotlib\n","\n","    # Build up the colors map and plot the figures\n","    fig = plt.figure(figsize=(10, 8))\n","    xticks = list(range(int(xs.min()), int(xs.max()), 1))\n","    yticks = list(range(int(ys.min()), int(ys.max()), 1))\n","    zticks = list(range(int(zs.min()), int(zs.max()), 1))\n","\n","    # Plot the data\n","    ax = fig.add_subplot(1, 1, 1, projection='3d')\n","    ax.scatter(xs, ys, zs, s=10, c = \"darkblue\", alpha=0.9, edgecolors='w', depthshade = False)\n","    ax.scatter(xp, yp, zp, s=10, c = \"red\", alpha=0.9, edgecolors='w', depthshade = False)\n","    ax.set_xticks(xticks)\n","    ax.set_yticks(yticks)\n","    ax.set_zticks(zticks)\n","    ax.set_xlabel('X')\n","    ax.set_ylabel('Y')\n","    ax.set_zlabel('Z')\n","    ax.set_title(title, fontdict = {\"fontsize\": 6})\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"gms14LQs8SfL"},"source":["**First and second components**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6IfO-f_28WTs"},"outputs":[],"source":["# Calculate the projection of x over the eigenvectors basis by only keeping the first and second components\n","components_array = [0, 1]\n","mu, z = pca_prin_comp(x = data.values,\n","                      eigen_vectors = eigen_vectors)\n","projection = pca_reconstruct(z = z[:, components_array], eigen_vectors = eigen_vectors[:, components_array], mean = mu)\n","error_projection_fs = np.sqrt(np.power(data.values - projection, 2).sum(axis = 1))\n","\n","# Plot the projection\n","plot_3d_and_projections(xs = data.x,\n","                        ys = data.y,\n","                        zs = data.z,\n","                        xp = projection[:, 0],\n","                        yp = projection[:, 1],\n","                        zp = projection[:, 2],\n","                        title = \"Real data versus projection over the first and second components\")"]},{"cell_type":"markdown","metadata":{"id":"Bsnw-yLZ8YlU"},"source":["**b) Keep the first and third principal components, project the points into a two dimensional vector space and calculate the projection error.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CmXf-QH88b95"},"outputs":[],"source":["# Calculate the projection of x over the eigenvectors basis by only keeping the first and second components\n","components_array = [0, 2]\n","mu, z = pca_prin_comp(x = data.values,\n","                      eigen_vectors = eigen_vectors)\n","projection = pca_reconstruct(z = z[:, components_array], eigen_vectors = eigen_vectors[:, components_array], mean = mu)\n","error_projection_ft = np.sqrt(np.power(data.values - projection, 2).sum(axis = 1))\n","\n","# Plot the projection\n","plot_3d_and_projections(xs = data.x,\n","                        ys = data.y,\n","                        zs = data.z,\n","                        xp = projection[:, 0],\n","                        yp = projection[:, 1],\n","                        zp = projection[:, 2],\n","                        title = \"Real data versus projection over the first and third components\")"]},{"cell_type":"markdown","metadata":{"id":"kUmjFNWh8jUN"},"source":["**5) Plot the low dimensional representative z in IR^2 and color them with the projection error. Look at the colorbars to compare the error in both plots.**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-t0Dyu998mhD"},"outputs":[],"source":["fig, (ax1, ax2) = plt.subplots(figsize=(10, 8), ncols=2)\n","sc = ax1.scatter(z[:, 0] , z[:, 1], s=10, c = error_projection_fs, alpha=0.9, edgecolors='w')\n","plt.colorbar(sc, ax = ax1)\n","ax1.set_title(\"First and second Principal Components\")\n","sc = ax2.scatter(z[:, 0] , z[:, 2], s=10, c = error_projection_ft, alpha=0.9, edgecolors='w')\n","plt.colorbar(sc, ax = ax2)\n","ax2.set_title(\"First and third Principal Components\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"y3asDamF8e4j"},"source":["<font color='blue'>**(QUESTION 2.4e)** Which 2D subspace approximates better the original points?"]},{"cell_type":"markdown","metadata":{"id":"OZgcnD-SemCN"},"source":["<font color='red'>**ANSWER**"]},{"cell_type":"markdown","metadata":{"id":"JmJgA3mr0jlb"},"source":["## Eigenfaces: application of PCA for face recognition"]},{"cell_type":"markdown","metadata":{"id":"1Xr_QpfC0mb_"},"source":["We are going to apply PCA to reduce the dimensionality in a face recognition\n","application. For this we are going to use the functions you completed.\n","\n","The objective is to recognize a person among a reduced set of\n","people, given a frontal picture of his face. The data we have is a data base of\n","$m$ images of faces from different persons, in different positions,\n","expressions and illumination conditions. The images are in gray scale, of size\n","$n=211\\times 219$.  We consider them as vectors in $\\mathbb R^{48319}$, a high\n","dimensional space."]},{"cell_type":"markdown","metadata":{"id":"1O-BiEZdUzdZ"},"source":["<figure>\n","  <img src=\"https://github.com/Muchay/OptTechCourse_Aux/blob/main/Lab1/images/faces_img.png?raw=true\" width=\"50%\">\n","  <figcaption>Figure 3: Some of the images used for the face recognition application.</figcaption>\n","</figure>\n","\n"]},{"cell_type":"markdown","metadata":{"id":"N9PRf-CT1YSn"},"source":["We will apply PCA to reduce the dimensionality of the space to $p$ (we will try\n","different values for $p$). We will proceed as follows."]},{"cell_type":"markdown","metadata":{"id":"xlQ85PkU1Zwc"},"source":["### Off-line stage: learning the principal directions"]},{"cell_type":"markdown","metadata":{"id":"MT26pD8V1kVr"},"source":["We consider now that $\\mathcal X = \\{x_1,\\dots,x_m\\}$ are the face images in\n","vector form.\n","\n","\n","1.   Compute the first $p$ eigenvectors and eigenvalues of $\\frac1{m-1}\\ma X^T\\ma X$. Let $\\ma V_p$ be the matrix whose columns are these eigenvectors (the principal directions)\n","2.   For each $x_i\\in\\mathcal X$, compute its $p$ first principal components:\n","This can be done with the following matrix multiplication: $$\\ma Z = \\ma X \\ma V_p,$$ where the $i$th row of $\\ma Z$, denoted by $z_i$, are the principal\n","components of the face $x_i$. $z_i$ is the low dimensional representative of $x_i.$"]},{"cell_type":"markdown","metadata":{"id":"1HYb3OSm2ZGq"},"source":["### On-line stage: recognition with principal components"]},{"cell_type":"markdown","metadata":{"id":"43CPL5u22fYz"},"source":["The objective now is to recognize a new face $x$. Again, $x$ is a grayscale\n","image of the face, considered as a vector. To simplify the algorithm, we assume\n","that $x$ is an image from a person of the dataset. Our recognizer is a very simple nearest\n","neighbor classifier.\n","\n","1.   Compute the principal components of $x$, $$z = [\\langle x, v_1\\rangle, \\langle x, v_2\\rangle, \\dots, \\langle x, v_p\\rangle].$$\n","\n","2. Look in the database for the face with nearest principal\tcomponents:\n","$$j^* = \\argmin_j \\|z - z_j\\|^2.$$\n"]},{"cell_type":"markdown","metadata":{"id":"u8W8MhR_Cit4"},"source":["<font color='blue'>**(QUESTION 2.5a)** The code for this part is provided. Run the provided functions and Read their code and comments and explain what each of these functions do, and the figures that open."]},{"cell_type":"markdown","metadata":{"id":"yxQwWeSjbvPW"},"source":["This is the first script of the face recognition application. The idea here is to run a very simple face recognition algorithm, but using PCA to reduce the dimensionality of the vectors. The original vectors are images of 211 x 229. If we consider the images as vectors in IR^n (each pixel is a component of the vector), we have that n = 221x229 = 48319. Using PCA, we will reduce the dimensionality to ~ 50, 1000 times smaller!!!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pH5hqZilbvPW"},"outputs":[],"source":["# In case needed:\n","#!conda install -c anaconda pillow -y"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y_8GyL2fbvPW"},"outputs":[],"source":["from PIL import Image\n","import os"]},{"cell_type":"markdown","metadata":{"id":"wB4fYFpsbvPX"},"source":["**1) Load faces from image files**\n","\n","Images are in data/faces folder. We will load them and store them as rows in a data matrix"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"i1THJsfibvPX"},"outputs":[],"source":["# List all images and read them. Once read, reshape them into (1, W x H)\n","faces_dir = os.path.abspath(source+\"data/faces\")\n","faces_files = [file_ for file_ in os.listdir(faces_dir) if file_.endswith(\"png\")]\n","faces_shape = np.array(Image.open(os.path.join(faces_dir, faces_files[0]))).shape\n","faces = np.empty((len(faces_files), faces_shape[0] * faces_shape[1]))\n","for index, img in enumerate(faces_files):\n","    im_frame = Image.open(os.path.join(faces_dir, img))\n","    np_frame = np.array(im_frame)\n","    faces[index] = np_frame.reshape((1, faces_shape[0] * faces_shape[1]))"]},{"cell_type":"markdown","metadata":{"id":"moq_ihRnbvPY"},"source":["**2) Split data randomly into a train set (tr), and a test set (ts)**\n","\n","Leave a part of the data set to test. The test data set are faces we will try to recognize.\n","\n","The sizes will be $80\\%$ for training and $20\\%$ for testing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AYdRL1KkbvPY"},"outputs":[],"source":["import random"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FlUipRc1bvPY"},"outputs":[],"source":["# Get the training and testing faces indexes\n","seed_value = None\n","np.random.seed(seed_value)\n","test_faces_idx = random.sample(list(range(len(faces))), k = int(len(faces) * 0.205))\n","train_faces_idx = list(set(list(range(len(faces)))).difference(set(test_faces_idx)))\n","\n","# Build up the training and testing faces\n","test_faces = faces[test_faces_idx, :]\n","train_faces = faces[train_faces_idx, :]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Q7zJTZ9tbvPY"},"outputs":[],"source":["len(train_faces)"]},{"cell_type":"markdown","metadata":{"id":"rVbZei8WbvPZ"},"source":["**3) Train PCA model**\n","\n","Now we will compute the principal directions using the training set.\n","\n","**Note:** the covariance matrix is too large to fit in memory. We can circumvent this problem using the SVD of x. Recall that the eigenvectors of $X^{T}X$ correspond to the right singular vectors (columns of $V$) satisfying:\n","\n","$$X = USV^{T}.$$\n","\n","So that,\n","\n","$$X^{T}X = VS^{T}U^{T}USV^{T} = VS^{T}SV^{T}.$$\n","\n","However, this does not work either. We have the same problem! The size of $V$ does not fit in memory. This is not surprising, both $X^{T}X$ and $V$ are n x n matrices.\n","\n","We can solve this problem with the economic size SVD. Note that $n - m$ columns of $V$ are multiplied by the zeros of $S$, in the product $USV^{T}$. This means that we can remove these columns. The economic size SVD is the following:\n","\n","$$X = US_0V_0^{T},$$\n","\n","where $S_0$ is $m$ x $m$ and $V_0$ is $n$ x $m$. The same can be done with the $U$ matrix in the case $m$ > $n$.\n","\n","We want to reduce $V$, not $U$. We can solve this problem very easily, by computing the svd of $X^{T}$. It can be shown that if\n","\n","$$X = USV^{T},$$\n","\n","is the SVD of $X$, then the svd of $X^{T}$ is given by\n","\n","$$X^{T} = VS^{T}U^{T}.$$\n","\n","The same happens with the economic size svd. Thus we will compute $V_0$ via the economic svd of $X^{T}$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"D_HMpXzkbvPZ"},"outputs":[],"source":["# Define P\n","p = len(train_faces)\n","\n","# Calculate the SVD of x' (x = x - mean(x))\n","U, S, Vt = np.linalg.svd((train_faces - train_faces.mean(axis = 0)).T, full_matrices=False)\n","\n","# Keep only the first p\n","V  = U[:, :p]\n","S = np.diag(S)[:p, :p]\n","\n","# The eigenvalues of x'*x are the square of the singular values\n","S = np.power(S, 2)"]},{"cell_type":"markdown","metadata":{"id":"N02oUikQbvPa"},"source":["**4) Visualize the PCA results**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fh0PLaoybvPa"},"outputs":[],"source":["# Calculate the participation in terms of %\n","tot = sum(np.diag(S))\n","var_exp = [(i / tot)*100 for i in sorted(np.diag(S), reverse=True)]\n","cum_var_exp = np.cumsum(var_exp)\n","\n","# Plot the graph\n","with plt.style.context('seaborn-whitegrid'):\n","    plt.figure(figsize=(8, 6))\n","    plt.bar(range(len(np.diag(S))), var_exp, alpha=0.5, align='center',\n","            label='individual explained variance')\n","    plt.step(range(len(np.diag(S))), cum_var_exp, where='mid',\n","             label='cumulative explained variance')\n","    plt.ylabel('Explained variance ratio')\n","    plt.xlabel('Principal components')\n","    plt.legend(loc='best')\n","    plt.tight_layout()\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"evREqTcJbvPa"},"source":["As you can see in graph above, by using the first 50 principal components we can keep almost the $90\\%$ of the total variance. Let's take a look at the *mean face*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0EoTOSDFbvPb"},"outputs":[],"source":["# Define the number of principal components to keep\n","p = 50\n","\n","# Keep only the first p principal directions\n","V_p = V[:, :p]\n","S_p = S[:p, :p]\n","\n","# Plot the mean face\n","plt.figure()\n","plt.imshow(train_faces.mean(axis = 0).reshape(faces_shape), cmap = \"gray\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"c9d_nzTgbvPb"},"source":["And now build a big image with the 6 first principal directions\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zuObXatLbvPb"},"outputs":[],"source":["# Build up the images array\n","num_rows = 2\n","num_columns = 3\n","tmp = np.empty(shape = (faces_shape[0] * num_rows, faces_shape[1] * num_columns))\n","component_index = 0\n","for row in range(num_rows):\n","    for column in range(num_columns):\n","        tmp[faces_shape[0] * row: faces_shape[0] * (row + 1), faces_shape[1] * column: faces_shape[1] * (column + 1)] = (S_p[component_index, component_index] * V_p[:, component_index]).reshape(faces_shape)\n","        component_index += 1\n","\n","# Plot the images\n","plt.figure()\n","plt.imshow(tmp, cmap = \"gray\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"JRVgUegYbvPc"},"source":["**5) project a test face over principal directions**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6SHdj_URbvPc"},"outputs":[],"source":["# Keep one of the test faces\n","x = test_faces[0, :]\n","\n","# Build up the principal components\n","_, z = pca_prin_comp(x = x, eigen_vectors = V_p, mu = train_faces.mean(axis = 0));\n","\n","# Reconstruct the face\n","x_proj = pca_reconstruct(z = z, eigen_vectors = V_p, mean = train_faces.mean(axis = 0))\n","\n","# Show both faces\n","fig, (ax1, ax2) = plt.subplots(figsize=(8, 6), ncols=2)\n","ax1.imshow(x.reshape(faces_shape), cmap = \"gray\")\n","ax1.set_title(\"Test image\")\n","ax2.imshow(x_proj.reshape(faces_shape), cmap = \"gray\")\n","ax2.set_title(\"Projected image\")\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"iIa3ONkmbvPc"},"source":["**6) Recognize faces**\n","\n","This a very simple face recognition algorithm: it is based on comparing the PCA coordinages of a query face with those of the training set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RptUooNQbvPd"},"outputs":[],"source":["# Import a library to simulate time steps\n","import time\n","%matplotlib inline\n","\n","# Define the number of principal components to keep\n","p = 50\n","\n","# Keep only the first p principal directions\n","V_p = V[:, :p]\n","S_p = S[:p, :p]\n","\n","# Compute principal components for training faces\n","_, train_z = pca_prin_comp(x = train_faces, eigen_vectors = V_p, mu = train_faces.mean(axis = 0))\n","\n","# Compute principal components for test faces\n","_, test_z = pca_prin_comp(x = test_faces, eigen_vectors = V_p, mu = train_faces.mean(axis = 0))\n","\n","# Classify\n","nn_idx = []\n","for i in range(len(test_faces)):\n","    d = np.linalg.norm(np.tile(test_z[[i], :], (train_z.shape[0], 1)).T - train_z.T, axis = 0).T\n","    nn_idx.append(int(d.argmin()))\n","\n","# Visualize classification results\n","plt.figure()\n","plt.ion()\n","for i in range(len(test_faces)):\n","    print(\"Showing image {} of {}\".format(i + 1, len(test_faces)))\n","    plt.subplot(1, 2, 1).imshow(test_faces[i, :].reshape(faces_shape), cmap = \"gray\")\n","    ax1.set_title(\"Test image\")\n","    plt.subplot(1, 2, 2).imshow(train_faces[nn_idx[i], :].reshape(faces_shape), cmap = \"gray\")\n","    ax2.set_title(\"Training image\")\n","    plt.show()\n","    plt.pause(1)"]},{"cell_type":"markdown","metadata":{"id":"1cZVQA9UgLVA"},"source":["<font color='red'>**ANSWER**"]},{"cell_type":"markdown","metadata":{"id":"5tOjvO-HfmAq"},"source":["<font color='blue'>**(QUESTION 2.5b)** Try changing the number of principal components used in step 6 and comment how this affects to the number of classification errors."]},{"cell_type":"markdown","metadata":{"id":"bf_tyrxh_cwd"},"source":["<font color='red'>**ANSWER**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vrrgQUPvnlXp"},"outputs":[],"source":["output.disable_custom_widget_manager()"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.2"}},"nbformat":4,"nbformat_minor":0}
